{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS OF YELP DATASET\n",
    "#### VISHAL BHARTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Import the libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from timeit import default_timer as timer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.sparse import vstack\n",
    "import itertools\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, BatchNormalization\n",
    "from keras.metrics import categorical_accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import word_tokenize\n",
    "import scipy.sparse\n",
    "from nltk.sentiment.util import mark_negation\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout, Bar\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the pre-processed data\n",
    "df_final = pd.read_pickle(\"final_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>vXMQhG5JtNwSRb6iql7iig</td>\n",
       "      <td>GK07iEy8UllYo113DlNnww</td>\n",
       "      <td>HrG_BxmOMPbqstycmzORzw</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>They did some transmission work for me that in...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>\"2630 E Bell Rd, Ste 4\"</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85032</td>\n",
       "      <td>33.640787</td>\n",
       "      <td>-112.025290</td>\n",
       "      <td>4.5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Transmission Repair;Automotive;Auto Repair;Aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254005</th>\n",
       "      <td>W_rauRWpM3ok_diREwEM2A</td>\n",
       "      <td>sjzv-c1k_HGGT9vZbfimWw</td>\n",
       "      <td>HONwpNQ2fmwMTOIZu0VI1A</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>Poor service when it comes to scheduling. Base...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>\"6045 S Rainbow Blvd\"</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89118</td>\n",
       "      <td>36.078796</td>\n",
       "      <td>-115.242975</td>\n",
       "      <td>1.5</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>Chiropractors;Health &amp; Medical;Family Practice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110374</th>\n",
       "      <td>h2F3EgNUdcggV8XrW2VQdg</td>\n",
       "      <td>KL-JE4VkGW02LzeSlW3e6Q</td>\n",
       "      <td>NWlNMG_eBIvDjCcHK46eDQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>Went on a Tuesday night, and it was really emp...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>\"4717 E Bell Rd\"</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85032</td>\n",
       "      <td>33.640232</td>\n",
       "      <td>-111.979545</td>\n",
       "      <td>3.5</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>Beer;Wine &amp; Spirits;Nightlife;Sports Bars;Bars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110373</th>\n",
       "      <td>L3r_OGsUqObVUEyP9uR_Bw</td>\n",
       "      <td>AlYZFOW_Xqi0qXelUrrHVw</td>\n",
       "      <td>NWlNMG_eBIvDjCcHK46eDQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>Meh\\n\\nI can imagine going here for the game, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>\"4717 E Bell Rd\"</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85032</td>\n",
       "      <td>33.640232</td>\n",
       "      <td>-111.979545</td>\n",
       "      <td>3.5</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>Beer;Wine &amp; Spirits;Nightlife;Sports Bars;Bars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110372</th>\n",
       "      <td>u0LWSgqpthGe3R5YlWR6mw</td>\n",
       "      <td>hNY3RdZK7dT43dznSxiA5A</td>\n",
       "      <td>NWlNMG_eBIvDjCcHK46eDQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-11-19</td>\n",
       "      <td>Went here last night for dinner. Service was l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>\"4717 E Bell Rd\"</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85032</td>\n",
       "      <td>33.640232</td>\n",
       "      <td>-111.979545</td>\n",
       "      <td>3.5</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>Beer;Wine &amp; Spirits;Nightlife;Sports Bars;Bars...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id                 user_id  \\\n",
       "299999  vXMQhG5JtNwSRb6iql7iig  GK07iEy8UllYo113DlNnww   \n",
       "254005  W_rauRWpM3ok_diREwEM2A  sjzv-c1k_HGGT9vZbfimWw   \n",
       "110374  h2F3EgNUdcggV8XrW2VQdg  KL-JE4VkGW02LzeSlW3e6Q   \n",
       "110373  L3r_OGsUqObVUEyP9uR_Bw  AlYZFOW_Xqi0qXelUrrHVw   \n",
       "110372  u0LWSgqpthGe3R5YlWR6mw  hNY3RdZK7dT43dznSxiA5A   \n",
       "\n",
       "                   business_id  stars_x       date  \\\n",
       "299999  HrG_BxmOMPbqstycmzORzw        1 2013-01-09   \n",
       "254005  HONwpNQ2fmwMTOIZu0VI1A        1 2014-03-15   \n",
       "110374  NWlNMG_eBIvDjCcHK46eDQ        2 2015-10-14   \n",
       "110373  NWlNMG_eBIvDjCcHK46eDQ        2 2015-12-13   \n",
       "110372  NWlNMG_eBIvDjCcHK46eDQ        2 2016-11-19   \n",
       "\n",
       "                                                     text  useful  funny  \\\n",
       "299999  They did some transmission work for me that in...       3      2   \n",
       "254005  Poor service when it comes to scheduling. Base...       0      0   \n",
       "110374  Went on a Tuesday night, and it was really emp...       2      1   \n",
       "110373  Meh\\n\\nI can imagine going here for the game, ...       3      2   \n",
       "110372  Went here last night for dinner. Service was l...       0      0   \n",
       "\n",
       "        cool    rating                        ...                          \\\n",
       "299999     1  negative                        ...                           \n",
       "254005     0  negative                        ...                           \n",
       "110374     1  negative                        ...                           \n",
       "110373     0  negative                        ...                           \n",
       "110372     0  negative                        ...                           \n",
       "\n",
       "                        address       city state postal_code   latitude  \\\n",
       "299999  \"2630 E Bell Rd, Ste 4\"    Phoenix    AZ       85032  33.640787   \n",
       "254005    \"6045 S Rainbow Blvd\"  Las Vegas    NV       89118  36.078796   \n",
       "110374         \"4717 E Bell Rd\"    Phoenix    AZ       85032  33.640232   \n",
       "110373         \"4717 E Bell Rd\"    Phoenix    AZ       85032  33.640232   \n",
       "110372         \"4717 E Bell Rd\"    Phoenix    AZ       85032  33.640232   \n",
       "\n",
       "         longitude  stars_y  review_count  is_open  \\\n",
       "299999 -112.025290      4.5            17        1   \n",
       "254005 -115.242975      1.5            56        0   \n",
       "110374 -111.979545      3.5           247        0   \n",
       "110373 -111.979545      3.5           247        0   \n",
       "110372 -111.979545      3.5           247        0   \n",
       "\n",
       "                                               categories  \n",
       "299999  Transmission Repair;Automotive;Auto Repair;Aut...  \n",
       "254005  Chiropractors;Health & Medical;Family Practice...  \n",
       "110374  Beer;Wine & Spirits;Nightlife;Sports Bars;Bars...  \n",
       "110373  Beer;Wine & Spirits;Nightlife;Sports Bars;Bars...  \n",
       "110372  Beer;Wine & Spirits;Nightlife;Sports Bars;Bars...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification into three classes 'positive', 'neutral' and 'negative'\n",
    "\n",
    "#### NOTE : \n",
    "* Each classifier takes around 10 mins to run for cases where the negations are not handled and round 60 mins when negations are handled (including the times for generating TFIDF vectors while handling negations). \n",
    "* To fetch the saved results, the cells following the classifier function call can be executed. This cells need not be run when running the classifiers, as the results are returned by the functions. The function calls that load the saved results assume that the 'Results' directory is inside current working directory.\n",
    "* The results in the write and those in some of these cells might have slight variation due to the random components in the algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams (U), stop words removed (SWR), punctuations removed (PR) (tfidfvectorizer default setting) and vary the feature size in 500, 1000, 1500, 2500 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note : Takes around 10 mins to run\n",
    "## Function that runs the classifiers on unigram features with stop words and punctuations removed\n",
    "def runClassifiersU(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        print(\"\\nRunning classifiers for feature size : {}\".format(MaxFeatures))\n",
    "        # Initialize the tfidf vectorizer \n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = MaxFeatures, norm='l2')\n",
    "        # Run the vectorizer on the yelp reviews\n",
    "        tokens = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        # Split into training and test set 70:30 split\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "        # if showRes is true the confusion matrix for each run is printed\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logistic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running classifiers for feature size : 500\n",
      "\n",
      "Running classifiers for feature size : 1000\n",
      "\n",
      "Running classifiers for feature size : 1500\n",
      "\n",
      "Running classifiers for feature size : 2500\n",
      "\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 673.2 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict, svmdict, lrdict, mlpdict = runClassifiersU()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "## Save the results\n",
    "# np.save(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\nbdict.npy\", nbdict)\n",
    "# np.save(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\svmdict.npy\", svmdict)\n",
    "# np.save(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\lrdict.npy\", lrdict)\n",
    "# np.save(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\mlpdict.npy\", mlpdict)\n",
    "\n",
    "## To load results from file. These are in the same format as the results from the above function call.\n",
    "nbdict = np.load(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\nbdict.npy\").item()\n",
    "svmdict = np.load(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\svmdict.npy\").item()\n",
    "lrdict = np.load(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\lrdict.npy\").item()\n",
    "mlpdict = np.load(\"D:\\\\Text Mining\\\\Final Project\\\\Results\\\\U_SWR_PR\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots for accuracy, training time, test time and f1-score for the used classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          69.5,
          70.8,
          71.1,
          71.7,
          72
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.2,
          72.89999999999999,
          73.6,
          74.5,
          75
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.6,
          73.3,
          73.9,
          74.5,
          74.7
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.8,
          74.5,
          75.3,
          76,
          76.2
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (U_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"9eac86e5-a805-4eb6-a698-68a387b64592\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9eac86e5-a805-4eb6-a698-68a387b64592\", [{\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Naive Bayes\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"type\": \"scatter\", \"y\": [69.5, 70.8, 71.1, 71.7, 72.0], \"legendgroup\": \"1\", \"mode\": \"lines+markers\"}, {\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"type\": \"scatter\", \"y\": [71.2, 72.89999999999999, 73.6, 74.5, 75.0], \"legendgroup\": \"2\", \"mode\": \"lines+markers\"}, {\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"type\": \"scatter\", \"y\": [71.6, 73.3, 73.9, 74.5, 74.7], \"legendgroup\": \"3\", \"mode\": \"lines+markers\"}, {\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"type\": \"scatter\", \"y\": [72.8, 74.5, 75.3, 76.0, 76.2], \"legendgroup\": \"4\", \"mode\": \"lines+markers\"}], {\"height\": 700, \"title\": \"Comparison of classification accuracy for different algorithms (U_SWR_PR)\", \"autosize\": false, \"hovermode\": \"closest\", \"width\": 1000, \"xaxis\": {\"title\": \"Feature size\", \"zeroline\": false, \"gridwidth\": 2, \"ticklen\": 5}, \"yaxis\": {\"title\": \"Accuracy in percentage\", \"range\": [40, 100], \"gridwidth\": 2, \"ticklen\": 5}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9eac86e5-a805-4eb6-a698-68a387b64592\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9eac86e5-a805-4eb6-a698-68a387b64592\", [{\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Naive Bayes\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"type\": \"scatter\", \"y\": [69.5, 70.8, 71.1, 71.7, 72.0], \"legendgroup\": \"1\", \"mode\": \"lines+markers\"}, {\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"type\": \"scatter\", \"y\": [71.2, 72.89999999999999, 73.6, 74.5, 75.0], \"legendgroup\": \"2\", \"mode\": \"lines+markers\"}, {\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"type\": \"scatter\", \"y\": [71.6, 73.3, 73.9, 74.5, 74.7], \"legendgroup\": \"3\", \"mode\": \"lines+markers\"}, {\"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"type\": \"scatter\", \"y\": [72.8, 74.5, 75.3, 76.0, 76.2], \"legendgroup\": \"4\", \"mode\": \"lines+markers\"}], {\"height\": 700, \"title\": \"Comparison of classification accuracy for different algorithms (U_SWR_PR)\", \"autosize\": false, \"hovermode\": \"closest\", \"width\": 1000, \"xaxis\": {\"title\": \"Feature size\", \"zeroline\": false, \"gridwidth\": 2, \"ticklen\": 5}, \"yaxis\": {\"title\": \"Accuracy in percentage\", \"range\": [40, 100], \"gridwidth\": 2, \"ticklen\": 5}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][0]*100, nbdict['1000'][0]*100, nbdict['1500'][0]*100, nbdict['2500'][0]*100, nbdict['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][0]*100, svmdict['1000'][0]*100, svmdict['1500'][0]*100, svmdict['2500'][0]*100, svmdict['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][0]*100, lrdict['1000'][0]*100, lrdict['1500'][0]*100, lrdict['2500'][0]*100, lrdict['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][0]*100, mlpdict['1000'][0]*100, mlpdict['1500'][0]*100, mlpdict['2500'][0]*100, mlpdict['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (U_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.27,
          0.28,
          0.28,
          0.29,
          0.3
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.94,
          1,
          1.11,
          1.08,
          1.13
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          1.22,
          1.31,
          1.33,
          1.39,
          1.45
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.95,
          83.72,
          90.59,
          101.45,
          115.93
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of training time for different algorithms (U_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Training time in seconds (log-scale)",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div id=\"d41f57f2-f128-4102-a4d0-3eceb261bd24\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d41f57f2-f128-4102-a4d0-3eceb261bd24\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.27, 0.28, 0.28, 0.29, 0.3], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [0.94, 1.0, 1.11, 1.08, 1.13], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [1.22, 1.31, 1.33, 1.39, 1.45], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [72.95, 83.72, 90.59, 101.45, 115.93], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"type\": \"log\", \"gridwidth\": 2, \"title\": \"Training time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of training time for different algorithms (U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d41f57f2-f128-4102-a4d0-3eceb261bd24\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d41f57f2-f128-4102-a4d0-3eceb261bd24\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.27, 0.28, 0.28, 0.29, 0.3], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [0.94, 1.0, 1.11, 1.08, 1.13], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [1.22, 1.31, 1.33, 1.39, 1.45], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [72.95, 83.72, 90.59, 101.45, 115.93], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"type\": \"log\", \"gridwidth\": 2, \"title\": \"Training time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of training time for different algorithms (U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE TRAINING TIME\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][4], nbdict['1000'][4], nbdict['1500'][4], nbdict['2500'][4], nbdict['5000'][4]],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    legendgroup=\"1\",\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    "    #5799C7\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][4], svmdict['1000'][4], svmdict['1500'][4], svmdict['2500'][4], svmdict['5000'][4]],\n",
    "    legendgroup=\"2\",\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM',\n",
    "    marker= {'color': 'rgb(255, 127, 14)'}\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][4], lrdict['1000'][4], lrdict['1500'][4], lrdict['2500'][4], lrdict['5000'][4]],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"3\",\n",
    "    name = 'Logistic Regression',\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][4], mlpdict['1000'][4], mlpdict['1500'][4], mlpdict['2500'][4], mlpdict['5000'][4]],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"4\",\n",
    "    name = 'Neural net',\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of training time for different algorithms (U_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Training time in seconds (log-scale)',\n",
    "        type='log',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.02,
          0.02,
          0.02,
          0.02,
          0.03
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.02,
          0.02,
          0.02,
          0.02,
          0.03
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.02,
          0.02,
          0.02,
          0.02,
          0.03
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.63,
          0.75,
          0.82,
          0.99,
          1.05
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of testing time for different algorithms (U+SWR+PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Testing time in seconds (log-scale)",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div id=\"92059ac2-bcc7-41ef-8564-da150aa3cf51\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"92059ac2-bcc7-41ef-8564-da150aa3cf51\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.02, 0.02, 0.02, 0.02, 0.03], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [0.02, 0.02, 0.02, 0.02, 0.03], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [0.02, 0.02, 0.02, 0.02, 0.03], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [0.63, 0.75, 0.82, 0.99, 1.05], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"type\": \"log\", \"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Testing time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of testing time for different algorithms (U+SWR+PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"92059ac2-bcc7-41ef-8564-da150aa3cf51\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"92059ac2-bcc7-41ef-8564-da150aa3cf51\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.02, 0.02, 0.02, 0.02, 0.03], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [0.02, 0.02, 0.02, 0.02, 0.03], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [0.02, 0.02, 0.02, 0.02, 0.03], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [0.63, 0.75, 0.82, 0.99, 1.05], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"type\": \"log\", \"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Testing time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of testing time for different algorithms (U+SWR+PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE TESTING TIME\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][5], nbdict['1000'][5], nbdict['1500'][5], nbdict['2500'][5], nbdict['5000'][5]],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][5], svmdict['1000'][5], svmdict['1500'][5], svmdict['2500'][5], svmdict['5000'][5]],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][5], lrdict['1000'][5], lrdict['1500'][5], lrdict['2500'][5], lrdict['5000'][5]],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][5], mlpdict['1000'][5], mlpdict['1500'][5], mlpdict['2500'][5], mlpdict['5000'][5]],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of testing time for different algorithms (U+SWR+PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Testing time in seconds (log-scale)',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        type='log'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          69.5,
          70.89999999999999,
          71.2,
          71.8,
          72.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          70.6,
          72.39999999999999,
          73.1,
          74.1,
          74.5
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.3,
          73,
          73.6,
          74.2,
          74.5
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.8,
          74.6,
          75.3,
          75.9,
          76.3
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (U+SWR+PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"8e1b906a-c632-42fd-8af1-5be6cf229904\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8e1b906a-c632-42fd-8af1-5be6cf229904\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [69.5, 70.89999999999999, 71.2, 71.8, 72.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [70.6, 72.39999999999999, 73.1, 74.1, 74.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.3, 73.0, 73.6, 74.2, 74.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [72.8, 74.6, 75.3, 75.9, 76.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (U+SWR+PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8e1b906a-c632-42fd-8af1-5be6cf229904\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8e1b906a-c632-42fd-8af1-5be6cf229904\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [69.5, 70.89999999999999, 71.2, 71.8, 72.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [70.6, 72.39999999999999, 73.1, 74.1, 74.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.3, 73.0, 73.6, 74.2, 74.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [72.8, 74.6, 75.3, 75.9, 76.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (U+SWR+PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-SCORE\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][2]*100, nbdict['1000'][3]*100, nbdict['1500'][3]*100, nbdict['2500'][3]*100, nbdict['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][3]*100, svmdict['1000'][3]*100, svmdict['1500'][3]*100, svmdict['2500'][3]*100, svmdict['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][3]*100, lrdict['1000'][3]*100, lrdict['1500'][3]*100, lrdict['2500'][3]*100, lrdict['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][3]*100, mlpdict['1000'][3]*100, mlpdict['1500'][3]*100, mlpdict['2500'][3]*100, mlpdict['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (U+SWR+PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams, stop words removed, punctuations removed (tfidfvectorizer default setting), setting the feature size to 5000 and varying the minimum document frequency threshold in 5,10,15 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiers_df(showRes = False):\n",
    "    df_array = [5,10,15,20]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for mdf in df_array:\n",
    "        print(\"Running classifiers with min document frequency threshold : {}\".format(mdf))\n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = 5000, norm='l2', min_df=mdf)\n",
    "        tokens = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers with min document frequency threshold : 5\n",
      "Running classifiers with min document frequency threshold : 10\n",
      "Running classifiers with min document frequency threshold : 15\n",
      "Running classifiers with min document frequency threshold : 20\n",
      "Finished in : 629.22 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict2, svmdict2, lrdict2, mlpdict2 = runClassifiers_df()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# np.save(\"Results\\\\U_SWR_PR_DF\\\\nbdict.npy\", nbdict2)\n",
    "# np.save(\"Results\\\\U_SWR_PR_DF\\\\svmdict.npy\", svmdict2)\n",
    "# np.save(\"Results\\\\U_SWR_PR_DF\\\\lrdict.npy\", lrdict2)\n",
    "# np.save(\"Results\\\\U_SWR_PR_DF\\\\mlpdict.npy\", mlpdict2)\n",
    "\n",
    "## to load results from file\n",
    "nbdict2 = np.load(\"Results\\\\U_SWR_PR_DF\\\\nbdict.npy\").item()\n",
    "svmdict2 = np.load(\"Results\\\\U_SWR_PR_DF\\\\svmdict.npy\").item()\n",
    "lrdict2 = np.load(\"Results\\\\U_SWR_PR_DF\\\\lrdict.npy\").item()\n",
    "mlpdict2 = np.load(\"Results\\\\U_SWR_PR_DF\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          72,
          72,
          72,
          72
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          74.8,
          74.9,
          74.7,
          74.9
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          74.7,
          74.8,
          74.7,
          74.8
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          76.1,
          76.1,
          76.2,
          76.2
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (U_SWR_PR_DF)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Minimum document frequency",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"1ecdc597-8f51-46bf-bd2a-3c952c4ac565\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1ecdc597-8f51-46bf-bd2a-3c952c4ac565\", [{\"type\": \"scatter\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [72.0, 72.0, 72.0, 72.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [74.8, 74.9, 74.7, 74.9], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [74.7, 74.8, 74.7, 74.8], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [76.1, 76.1, 76.2, 76.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1ecdc597-8f51-46bf-bd2a-3c952c4ac565\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1ecdc597-8f51-46bf-bd2a-3c952c4ac565\", [{\"type\": \"scatter\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [72.0, 72.0, 72.0, 72.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [74.8, 74.9, 74.7, 74.9], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [74.7, 74.8, 74.7, 74.8], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [76.1, 76.1, 76.2, 76.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict2['5'][0]*100, nbdict2['10'][0]*100, nbdict2['15'][0]*100, nbdict2['20'][0]*100],\n",
    "    x = [5,10,15,20],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [svmdict2['5'][0]*100, svmdict2['10'][0]*100, svmdict2['15'][0]*100, svmdict2['20'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [lrdict2['5'][0]*100, lrdict2['10'][0]*100, lrdict2['15'][0]*100, lrdict2['20'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [mlpdict2['5'][0]*100, mlpdict2['10'][0]*100, mlpdict2['15'][0]*100, mlpdict2['20'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (U_SWR_PR_DF)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Minimum document frequency',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          72.2,
          72.2,
          72.2,
          72.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          74.4,
          74.4,
          74.2,
          74.5
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          74.4,
          74.5,
          74.4,
          74.5
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          76.1,
          76,
          76.3,
          76.2
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (U_SWR_PR_DF)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Minimum document frequency",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"a3263d12-07a6-4524-9dcb-a1bcfaf700a2\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"a3263d12-07a6-4524-9dcb-a1bcfaf700a2\", [{\"type\": \"scatter\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [72.2, 72.2, 72.2, 72.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [74.4, 74.4, 74.2, 74.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [74.4, 74.5, 74.4, 74.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [76.1, 76.0, 76.3, 76.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"a3263d12-07a6-4524-9dcb-a1bcfaf700a2\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"a3263d12-07a6-4524-9dcb-a1bcfaf700a2\", [{\"type\": \"scatter\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [72.2, 72.2, 72.2, 72.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [74.4, 74.4, 74.2, 74.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [74.4, 74.5, 74.4, 74.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [76.1, 76.0, 76.3, 76.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict2['5'][3]*100, nbdict2['10'][3]*100, nbdict2['15'][3]*100, nbdict2['20'][3]*100],\n",
    "    x = [5,10,15,20],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [svmdict2['5'][3]*100, svmdict2['10'][3]*100, svmdict2['15'][3]*100, svmdict2['20'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [lrdict2['5'][3]*100, lrdict2['10'][3]*100, lrdict2['15'][3]*100, lrdict2['20'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [mlpdict2['5'][3]*100, mlpdict2['10'][3]*100, mlpdict2['15'][3]*100, mlpdict2['20'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (U_SWR_PR_DF)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Minimum document frequency',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        range=[40,100],\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams (U), stop words removed(SWR) and vary the feature size in 500, 1000, 1500, 2500 and 5000, while also handling negations(HN).\n",
    "\n",
    "**NOTE**: The mark negation function takes about 10 mins to run for each feature size. For 5 feature sizes it takes around 50 mins to get the tfidf vectors with negations marked and another 10 mins to run the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 588.45 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 500, norm='l2')\n",
    "tokens500 = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 575.82 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1000, norm='l2')\n",
    "tokens1000 = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 577.43 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1500, norm='l2')\n",
    "tokens1500 = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 580.49 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 2500, norm='l2')\n",
    "tokens2500 = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 577.16 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 5000, norm='l2')\n",
    "tokens5000 = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersUMN(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        if MaxFeatures == 500:\n",
    "            tokens = tokens500\n",
    "        elif MaxFeatures == 1000:\n",
    "            tokens = tokens1000\n",
    "        elif MaxFeatures == 1500:\n",
    "            tokens = tokens1500\n",
    "        elif MaxFeatures == 2500:\n",
    "            tokens = tokens2500\n",
    "        elif MaxFeatures == 5000:\n",
    "            tokens = tokens5000\n",
    "        \n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "                \n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 562.18 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict3, svmdict3, lrdict3, mlpdict3 = runClassifiersUMN()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "## Save the results\n",
    "# np.save(\"Results\\\\U_SWR_HN\\\\nbdict.npy\", nbdict3)\n",
    "# np.save(\"Results\\\\U_SWR_HN\\\\svmdict.npy\", svmdict3)\n",
    "# np.save(\"Results\\\\U_SWR_HN\\\\lrdict.npy\", lrdict3)\n",
    "# np.save(\"Results\\\\U_SWR_HN\\\\mlpdict.npy\", mlpdict3)\n",
    "\n",
    "## to load results from file\n",
    "nbdict3 = np.load(\"Results\\\\U_SWR_HN\\\\nbdict.npy\").item()\n",
    "svmdict3 = np.load(\"Results\\\\U_SWR_HN\\\\svmdict.npy\").item()\n",
    "lrdict3 = np.load(\"Results\\\\U_SWR_HN\\\\lrdict.npy\").item()\n",
    "mlpdict3 = np.load(\"Results\\\\U_SWR_HN\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          68.4,
          70.19999999999999,
          70.8,
          71.39999999999999,
          71.89999999999999
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.5,
          74.2,
          75.1,
          75.6,
          76.4
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.39999999999999,
          74.3,
          74.8,
          75.4,
          75.8
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          74,
          76.1,
          76.7,
          77.2,
          77.5
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (U_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"f56a61c1-29fb-4d43-8107-5f8128157722\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f56a61c1-29fb-4d43-8107-5f8128157722\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [68.4, 70.19999999999999, 70.8, 71.39999999999999, 71.89999999999999], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [72.5, 74.2, 75.1, 75.6, 76.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [72.39999999999999, 74.3, 74.8, 75.4, 75.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [74.0, 76.1, 76.7, 77.2, 77.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (U_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"f56a61c1-29fb-4d43-8107-5f8128157722\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f56a61c1-29fb-4d43-8107-5f8128157722\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [68.4, 70.19999999999999, 70.8, 71.39999999999999, 71.89999999999999], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [72.5, 74.2, 75.1, 75.6, 76.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [72.39999999999999, 74.3, 74.8, 75.4, 75.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [74.0, 76.1, 76.7, 77.2, 77.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (U_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict3['500'][0]*100, nbdict3['1000'][0]*100, nbdict3['1500'][0]*100, nbdict3['2500'][0]*100, nbdict3['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict3['500'][0]*100, svmdict3['1000'][0]*100, svmdict3['1500'][0]*100, svmdict3['2500'][0]*100, svmdict3['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict3['500'][0]*100, lrdict3['1000'][0]*100, lrdict3['1500'][0]*100, lrdict3['2500'][0]*100, lrdict3['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict3['500'][0]*100, mlpdict3['1000'][0]*100, mlpdict3['1500'][0]*100, mlpdict3['2500'][0]*100, mlpdict3['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (U_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          68.4,
          70.1,
          70.7,
          71.3,
          71.89999999999999
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.2,
          73.7,
          74.7,
          75.1,
          76.1
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.2,
          74.2,
          74.6,
          75.2,
          75.6
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          74,
          76.1,
          76.7,
          77.10000000000001,
          77.5
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (U_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"51e0ace6-4786-4d57-9ad9-293bf53b974b\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"51e0ace6-4786-4d57-9ad9-293bf53b974b\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [68.4, 70.1, 70.7, 71.3, 71.89999999999999], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [72.2, 73.7, 74.7, 75.1, 76.1], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [72.2, 74.2, 74.6, 75.2, 75.6], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [74.0, 76.1, 76.7, 77.10000000000001, 77.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification F1-score for different algorithms (U_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"51e0ace6-4786-4d57-9ad9-293bf53b974b\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"51e0ace6-4786-4d57-9ad9-293bf53b974b\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [68.4, 70.1, 70.7, 71.3, 71.89999999999999], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [72.2, 73.7, 74.7, 75.1, 76.1], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [72.2, 74.2, 74.6, 75.2, 75.6], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [74.0, 76.1, 76.7, 77.10000000000001, 77.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification F1-score for different algorithms (U_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-SCORE\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict3['500'][2]*100, nbdict3['1000'][3]*100, nbdict3['1500'][3]*100, nbdict3['2500'][3]*100, nbdict3['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict3['500'][3]*100, svmdict3['1000'][3]*100, svmdict3['1500'][3]*100, svmdict3['2500'][3]*100, svmdict3['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict3['500'][3]*100, lrdict3['1000'][3]*100, lrdict3['1500'][3]*100, lrdict3['2500'][3]*100, lrdict3['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict3['500'][3]*100, mlpdict3['1000'][3]*100, mlpdict3['1500'][3]*100, mlpdict3['2500'][3]*100, mlpdict3['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (U_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with bigrams(only) (B), stop words removed (SWR), punctuations removed (tfidfvectorizer default setting) (PR) and vary the feature size in 500, 1000, 1500, 2500 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersB(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = MaxFeatures, ngram_range=(2, 2), norm='l2')\n",
    "        tokens = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logistic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 694.44 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict4, svmdict4, lrdict4, mlpdict4 = runClassifiersB()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "## Save the results\n",
    "# np.save(\"Results\\\\B_SWR_PR\\\\nbdict.npy\", nbdict4)\n",
    "# np.save(\"Results\\\\B_SWR_PR\\\\svmdict.npy\", svmdict4)\n",
    "# np.save(\"Results\\\\B_SWR_PR\\\\lrdict.npy\", lrdict4)\n",
    "# np.save(\"Results\\\\B_SWR_PR\\\\mlpdict.npy\", mlpdict4)\n",
    "\n",
    "# to load results from file\n",
    "nbdict4 = np.load(\"Results\\\\B_SWR_PR\\\\nbdict.npy\").item()\n",
    "svmdict4 = np.load(\"Results\\\\B_SWR_PR\\\\svmdict.npy\").item()\n",
    "lrdict4 = np.load(\"Results\\\\B_SWR_PR\\\\lrdict.npy\").item()\n",
    "mlpdict4 = np.load(\"Results\\\\B_SWR_PR\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          57.3,
          60.699999999999996,
          62.5,
          64.60000000000001,
          67.30000000000001
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          55.800000000000004,
          59.5,
          61.4,
          63.7,
          66.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.699999999999996,
          60.3,
          62.1,
          64.3,
          66.60000000000001
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          57.099999999999994,
          60.8,
          62.5,
          64.8,
          67.5
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (B_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"fd0a4e72-1060-4537-94c8-fc9339405c5f\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fd0a4e72-1060-4537-94c8-fc9339405c5f\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [57.3, 60.699999999999996, 62.5, 64.60000000000001, 67.30000000000001], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [55.800000000000004, 59.5, 61.4, 63.7, 66.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [56.699999999999996, 60.3, 62.1, 64.3, 66.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [57.099999999999994, 60.8, 62.5, 64.8, 67.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (B_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"fd0a4e72-1060-4537-94c8-fc9339405c5f\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"fd0a4e72-1060-4537-94c8-fc9339405c5f\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [57.3, 60.699999999999996, 62.5, 64.60000000000001, 67.30000000000001], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [55.800000000000004, 59.5, 61.4, 63.7, 66.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [56.699999999999996, 60.3, 62.1, 64.3, 66.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [57.099999999999994, 60.8, 62.5, 64.8, 67.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (B_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict4['500'][0]*100, nbdict4['1000'][0]*100, nbdict4['1500'][0]*100, nbdict4['2500'][0]*100, nbdict4['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict4['500'][0]*100, svmdict4['1000'][0]*100, svmdict4['1500'][0]*100, svmdict4['2500'][0]*100, svmdict4['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict4['500'][0]*100, lrdict4['1000'][0]*100, lrdict4['1500'][0]*100, lrdict4['2500'][0]*100, lrdict4['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict4['500'][0]*100, mlpdict4['1000'][0]*100, mlpdict4['1500'][0]*100, mlpdict4['2500'][0]*100, mlpdict4['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (B_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,    \n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        range=[40,100],\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.699999999999996,
          60.3,
          62.2,
          64.4,
          67.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          54.900000000000006,
          58.599999999999994,
          60.699999999999996,
          62.9,
          65.4
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.3,
          60,
          61.9,
          64,
          66.3
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.699999999999996,
          60.5,
          62.2,
          64.60000000000001,
          67.2
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification f1-score for different algorithms (B_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"2d9cd21d-350f-4831-a42b-83abacb852f5\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2d9cd21d-350f-4831-a42b-83abacb852f5\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [56.699999999999996, 60.3, 62.2, 64.4, 67.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [54.900000000000006, 58.599999999999994, 60.699999999999996, 62.9, 65.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [56.3, 60.0, 61.9, 64.0, 66.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [56.699999999999996, 60.5, 62.2, 64.60000000000001, 67.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification f1-score for different algorithms (B_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"2d9cd21d-350f-4831-a42b-83abacb852f5\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2d9cd21d-350f-4831-a42b-83abacb852f5\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [56.699999999999996, 60.3, 62.2, 64.4, 67.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [54.900000000000006, 58.599999999999994, 60.699999999999996, 62.9, 65.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [56.3, 60.0, 61.9, 64.0, 66.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [56.699999999999996, 60.5, 62.2, 64.60000000000001, 67.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification f1-score for different algorithms (B_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict4['500'][3]*100, nbdict4['1000'][3]*100, nbdict4['1500'][3]*100, nbdict4['2500'][3]*100, nbdict4['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict4['500'][3]*100, svmdict4['1000'][3]*100, svmdict4['1500'][3]*100, svmdict4['2500'][3]*100, svmdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict4['500'][3]*100, lrdict4['1000'][3]*100, lrdict4['1500'][3]*100, lrdict4['2500'][3]*100, lrdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict4['500'][3]*100, mlpdict4['1000'][3]*100, mlpdict4['1500'][3]*100, mlpdict4['2500'][3]*100, mlpdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification f1-score for different algorithms (B_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with bigrams(only) (B), stop words removed (SWR) and vary the feature size in 500, 1000, 1500, 2500 and 5000, while handling negations (HN).\n",
    "\n",
    "**NOTE**: The mark negation function takes about 10 mins to run for each feature size. For 5 feature sizes it takes around 50 mins to get the tfidf vectors with negations marked and another 10 mins to run the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 632.1 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 500, ngram_range=(2, 2), norm='l2')\n",
    "tokens500B = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 642.98 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1000, ngram_range=(2, 2), norm='l2')\n",
    "tokens1000B = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 644.36 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1500, ngram_range=(2, 2), norm='l2')\n",
    "tokens1500B = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 624.81 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 2500, ngram_range=(2, 2), norm='l2')\n",
    "tokens2500B = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 625.79 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 5000, ngram_range=(2, 2), norm='l2')\n",
    "tokens5000B = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersBN(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        if MaxFeatures == 500:\n",
    "            tokens = tokens500B\n",
    "        elif MaxFeatures == 1000:\n",
    "            tokens = tokens1000B\n",
    "        elif MaxFeatures == 1500:\n",
    "            tokens = tokens1500B\n",
    "        elif MaxFeatures == 2500:\n",
    "            tokens = tokens2500B\n",
    "        elif MaxFeatures == 5000:\n",
    "            tokens = tokens5000B\n",
    "            \n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "                \n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 374.73 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict5, svmdict5, lrdict5, mlpdict5 = runClassifiersBN()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.save(\"Results\\\\B_SWR_HN\\\\nbdict.npy\", nbdict5)\n",
    "# np.save(\"Results\\\\B_SWR_HN\\\\svmdict.npy\", svmdict5)\n",
    "# np.save(\"Results\\\\B_SWR_HN\\\\lrdict.npy\", lrdict5)\n",
    "# np.save(\"Results\\\\B_SWR_HN\\\\mlpdict.npy\", mlpdict5)\n",
    "\n",
    "## to load results from file\n",
    "nbdict5 = np.load(\"Results\\\\B_SWR_HN\\\\nbdict.npy\").item()\n",
    "svmdict5 = np.load(\"Results\\\\B_SWR_HN\\\\svmdict.npy\").item()\n",
    "lrdict5 = np.load(\"Results\\\\B_SWR_HN\\\\lrdict.npy\").item()\n",
    "mlpdict5 = np.load(\"Results\\\\B_SWR_HN\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          62.6,
          66.60000000000001,
          68.2,
          70.19999999999999,
          72.3
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          62.9,
          67.10000000000001,
          68.89999999999999,
          71.2,
          73.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          63.800000000000004,
          67.60000000000001,
          69.19999999999999,
          71.2,
          73
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          64.3,
          68.4,
          70.19999999999999,
          72.39999999999999,
          74.3
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (B_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"f1218505-d006-4dea-964a-532596ea8e64\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f1218505-d006-4dea-964a-532596ea8e64\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [62.6, 66.60000000000001, 68.2, 70.19999999999999, 72.3], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [62.9, 67.10000000000001, 68.89999999999999, 71.2, 73.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [63.800000000000004, 67.60000000000001, 69.19999999999999, 71.2, 73.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [64.3, 68.4, 70.19999999999999, 72.39999999999999, 74.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (B_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"f1218505-d006-4dea-964a-532596ea8e64\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f1218505-d006-4dea-964a-532596ea8e64\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [62.6, 66.60000000000001, 68.2, 70.19999999999999, 72.3], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [62.9, 67.10000000000001, 68.89999999999999, 71.2, 73.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [63.800000000000004, 67.60000000000001, 69.19999999999999, 71.2, 73.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [64.3, 68.4, 70.19999999999999, 72.39999999999999, 74.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (B_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict5['500'][0]*100, nbdict5['1000'][0]*100, nbdict5['1500'][0]*100, nbdict5['2500'][0]*100, nbdict5['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict5['500'][0]*100, svmdict5['1000'][0]*100, svmdict5['1500'][0]*100, svmdict5['2500'][0]*100, svmdict5['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict5['500'][0]*100, lrdict5['1000'][0]*100, lrdict5['1500'][0]*100, lrdict5['2500'][0]*100, lrdict5['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict5['500'][0]*100, mlpdict5['1000'][0]*100, mlpdict5['1500'][0]*100, mlpdict5['2500'][0]*100, mlpdict5['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (B_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          62.4,
          66.5,
          68.10000000000001,
          70.1,
          72.3
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          62.1,
          66.60000000000001,
          68.30000000000001,
          70.7,
          72.8
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          63.5,
          67.4,
          69,
          70.89999999999999,
          72.8
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          64.1,
          68.2,
          70.1,
          72.3,
          74.2
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (B_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"35952f8e-9be6-4c7e-afa5-7ecfe2d4e95c\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"35952f8e-9be6-4c7e-afa5-7ecfe2d4e95c\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [62.4, 66.5, 68.10000000000001, 70.1, 72.3], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [62.1, 66.60000000000001, 68.30000000000001, 70.7, 72.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [63.5, 67.4, 69.0, 70.89999999999999, 72.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [64.1, 68.2, 70.1, 72.3, 74.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification F1-score for different algorithms (B_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"35952f8e-9be6-4c7e-afa5-7ecfe2d4e95c\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"35952f8e-9be6-4c7e-afa5-7ecfe2d4e95c\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [62.4, 66.5, 68.10000000000001, 70.1, 72.3], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [62.1, 66.60000000000001, 68.30000000000001, 70.7, 72.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [63.5, 67.4, 69.0, 70.89999999999999, 72.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [64.1, 68.2, 70.1, 72.3, 74.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification F1-score for different algorithms (B_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict5['500'][3]*100, nbdict5['1000'][3]*100, nbdict5['1500'][3]*100, nbdict5['2500'][3]*100, nbdict5['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict5['500'][3]*100, svmdict5['1000'][3]*100, svmdict5['1500'][3]*100, svmdict5['2500'][3]*100, svmdict5['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict5['500'][3]*100, lrdict5['1000'][3]*100, lrdict5['1500'][3]*100, lrdict5['2500'][3]*100, lrdict5['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict5['500'][3]*100, mlpdict5['1000'][3]*100, mlpdict5['1500'][3]*100, mlpdict5['2500'][3]*100, mlpdict5['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (B_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams+bigrams (UB), stop words removed (SWR), punctuations removed (tfidfvectorizer default setting) (PR) and vary the feature size in 500, 1000, 1500, 2500 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersUB(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = MaxFeatures, ngram_range=(1, 2), norm='l2')\n",
    "        tokens = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 972.61 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict6, svmdict6, lrdict6, mlpdict6 = runClassifiersUB()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.save(\"Results\\\\UB_SWR_PR\\\\nbdict.npy\", nbdict6)\n",
    "# np.save(\"Results\\\\UB_SWR_PR\\\\svmdict.npy\", svmdict6)\n",
    "# np.save(\"Results\\\\UB_SWR_PR\\\\lrdict.npy\", lrdict6)\n",
    "# np.save(\"Results\\\\UB_SWR_PR\\\\mlpdict.npy\", mlpdict6)\n",
    "\n",
    "## to load results from file\n",
    "nbdict6 = np.load(\"Results\\\\UB_SWR_PR\\\\nbdict.npy\").item()\n",
    "svmdict6 = np.load(\"Results\\\\UB_SWR_PR\\\\svmdict.npy\").item()\n",
    "lrdict6 = np.load(\"Results\\\\UB_SWR_PR\\\\lrdict.npy\").item()\n",
    "mlpdict6 = np.load(\"Results\\\\UB_SWR_PR\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          69.39999999999999,
          70.8,
          71.3,
          72,
          72.7
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.1,
          73.1,
          74.1,
          74.7,
          75.4
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.6,
          73.3,
          74.1,
          74.6,
          75.1
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72.8,
          74.6,
          75.4,
          76.2,
          76.8
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (UB_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"694789c8-5695-40d0-bc42-53f1eabe97cb\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"694789c8-5695-40d0-bc42-53f1eabe97cb\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [69.39999999999999, 70.8, 71.3, 72.0, 72.7], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [71.1, 73.1, 74.1, 74.7, 75.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.6, 73.3, 74.1, 74.6, 75.1], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [72.8, 74.6, 75.4, 76.2, 76.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (UB_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"694789c8-5695-40d0-bc42-53f1eabe97cb\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"694789c8-5695-40d0-bc42-53f1eabe97cb\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [69.39999999999999, 70.8, 71.3, 72.0, 72.7], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [71.1, 73.1, 74.1, 74.7, 75.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.6, 73.3, 74.1, 74.6, 75.1], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [72.8, 74.6, 75.4, 76.2, 76.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (UB_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict6['500'][0]*100, nbdict6['1000'][0]*100, nbdict6['1500'][0]*100, nbdict6['2500'][0]*100, nbdict6['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict6['500'][0]*100, svmdict6['1000'][0]*100, svmdict6['1500'][0]*100, svmdict6['2500'][0]*100, svmdict6['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict6['500'][0]*100, lrdict6['1000'][0]*100, lrdict6['1500'][0]*100, lrdict6['2500'][0]*100, lrdict6['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict6['500'][0]*100, mlpdict6['1000'][0]*100, mlpdict6['1500'][0]*100, mlpdict6['2500'][0]*100, mlpdict6['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (UB_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.699999999999996,
          60.3,
          62.2,
          64.4,
          67.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          54.900000000000006,
          58.599999999999994,
          60.699999999999996,
          62.9,
          65.4
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.3,
          60,
          61.9,
          64,
          66.3
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          56.699999999999996,
          60.5,
          62.2,
          64.60000000000001,
          67.2
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification f1-score for different algorithms (UB_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"53bad255-af2f-4503-89b4-884c9798845e\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"53bad255-af2f-4503-89b4-884c9798845e\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [56.699999999999996, 60.3, 62.2, 64.4, 67.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [54.900000000000006, 58.599999999999994, 60.699999999999996, 62.9, 65.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [56.3, 60.0, 61.9, 64.0, 66.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [56.699999999999996, 60.5, 62.2, 64.60000000000001, 67.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification f1-score for different algorithms (UB_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"53bad255-af2f-4503-89b4-884c9798845e\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"53bad255-af2f-4503-89b4-884c9798845e\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [56.699999999999996, 60.3, 62.2, 64.4, 67.2], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [54.900000000000006, 58.599999999999994, 60.699999999999996, 62.9, 65.4], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [56.3, 60.0, 61.9, 64.0, 66.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [56.699999999999996, 60.5, 62.2, 64.60000000000001, 67.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification f1-score for different algorithms (UB_SWR_PR)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict4['500'][3]*100, nbdict4['1000'][3]*100, nbdict4['1500'][3]*100, nbdict4['2500'][3]*100, nbdict4['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict4['500'][3]*100, svmdict4['1000'][3]*100, svmdict4['1500'][3]*100, svmdict4['2500'][3]*100, svmdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict4['500'][3]*100, lrdict4['1000'][3]*100, lrdict4['1500'][3]*100, lrdict4['2500'][3]*100, lrdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict4['500'][3]*100, mlpdict4['1000'][3]*100, mlpdict4['1500'][3]*100, mlpdict4['2500'][3]*100, mlpdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification f1-score for different algorithms (UB_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams+bigrams (UB), stop words removed (SWR) and vary the feature size in 500, 1000, 1500, 2500 and 5000, while handling the negations (HN).\n",
    "\n",
    "**NOTE**: The mark negation function takes a about 10 mins to run for each feature size. For 5 feature sizes it takes around 50 mins to get the tfidf vectors with negations marked and another 10 mins to run the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 650.57 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 500, ngram_range=(1, 2), norm='l2')\n",
    "tokens500UB = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 646.99 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1000, ngram_range=(1, 2), norm='l2')\n",
    "tokens1000UB = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 668.92 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1500, ngram_range=(1, 2), norm='l2')\n",
    "tokens1500UB = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 663.37 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 2500, ngram_range=(1, 2), norm='l2')\n",
    "tokens2500UB = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 660.59 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 5000, ngram_range=(1, 2), norm='l2')\n",
    "tokens5000UB = tfidf_vect.fit_transform(df_final[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersUBN(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        if MaxFeatures == 500:\n",
    "            tokens = tokens500UB\n",
    "        elif MaxFeatures == 1000:\n",
    "            tokens = tokens1000UB\n",
    "        elif MaxFeatures == 1500:\n",
    "            tokens = tokens1500UB\n",
    "        elif MaxFeatures == 2500:\n",
    "            tokens = tokens2500UB\n",
    "        elif MaxFeatures == 5000:\n",
    "            tokens = tokens5000UB\n",
    "            \n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "                \n",
    "        labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000], tokens[200000:270000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000], tokens[270000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(3, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 593.92 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict7, svmdict7, lrdict7, mlpdict7 = runClassifiersUBN()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.save(\"Results\\\\UB_SWR_HN\\\\nbdict.npy\", nbdict7)\n",
    "# np.save(\"Results\\\\UB_SWR_HN\\\\svmdict.npy\", svmdict7)\n",
    "# np.save(\"Results\\\\UB_SWR_HN\\\\lrdict.npy\", lrdict7)\n",
    "# np.save(\"Results\\\\UB_SWR_HN\\\\mlpdict.npy\", mlpdict7)\n",
    "\n",
    "## to load results from file\n",
    "nbdict7 = np.load(\"Results\\\\UB_SWR_HN\\\\nbdict.npy\").item()\n",
    "svmdict7 = np.load(\"Results\\\\UB_SWR_HN\\\\svmdict.npy\").item()\n",
    "lrdict7 = np.load(\"Results\\\\UB_SWR_HN\\\\lrdict.npy\").item()\n",
    "mlpdict7 = np.load(\"Results\\\\UB_SWR_HN\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          67.9,
          69.89999999999999,
          70.8,
          71.89999999999999,
          73
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          72,
          74.1,
          74.9,
          76.1,
          76.8
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.89999999999999,
          74.3,
          74.9,
          75.5,
          76.2
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          73.5,
          75.7,
          76.7,
          77.5,
          78.10000000000001
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (UB_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"c24d54d2-13e5-4256-91c5-fa36b781fad7\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c24d54d2-13e5-4256-91c5-fa36b781fad7\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [67.9, 69.89999999999999, 70.8, 71.89999999999999, 73.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [72.0, 74.1, 74.9, 76.1, 76.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.89999999999999, 74.3, 74.9, 75.5, 76.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [73.5, 75.7, 76.7, 77.5, 78.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (UB_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c24d54d2-13e5-4256-91c5-fa36b781fad7\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c24d54d2-13e5-4256-91c5-fa36b781fad7\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [67.9, 69.89999999999999, 70.8, 71.89999999999999, 73.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [72.0, 74.1, 74.9, 76.1, 76.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.89999999999999, 74.3, 74.9, 75.5, 76.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [73.5, 75.7, 76.7, 77.5, 78.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification accuracy for different algorithms (UB_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict7['500'][0]*100, nbdict7['1000'][0]*100, nbdict7['1500'][0]*100, nbdict7['2500'][0]*100, nbdict7['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict7['500'][0]*100, svmdict7['1000'][0]*100, svmdict7['1500'][0]*100, svmdict7['2500'][0]*100, svmdict7['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict7['500'][0]*100, lrdict7['1000'][0]*100, lrdict7['1500'][0]*100, lrdict7['2500'][0]*100, lrdict7['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict7['500'][0]*100, mlpdict7['1000'][0]*100, mlpdict7['1500'][0]*100, mlpdict7['2500'][0]*100, mlpdict7['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (UB_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          67.80000000000001,
          69.8,
          70.8,
          71.89999999999999,
          73
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.7,
          73.8,
          74.5,
          75.9,
          76.5
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          71.7,
          74.1,
          74.7,
          75.3,
          76.1
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          73.4,
          75.7,
          76.7,
          77.5,
          78.10000000000001
         ]
        }
       ],
       "layout": {
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification f1-score for different algorithms (UB_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"be94f8fe-67ca-44af-8bc1-fe59db212b44\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"be94f8fe-67ca-44af-8bc1-fe59db212b44\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [67.80000000000001, 69.8, 70.8, 71.89999999999999, 73.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [71.7, 73.8, 74.5, 75.9, 76.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.7, 74.1, 74.7, 75.3, 76.1], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [73.4, 75.7, 76.7, 77.5, 78.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification f1-score for different algorithms (UB_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"be94f8fe-67ca-44af-8bc1-fe59db212b44\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"be94f8fe-67ca-44af-8bc1-fe59db212b44\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [67.80000000000001, 69.8, 70.8, 71.89999999999999, 73.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [71.7, 73.8, 74.5, 75.9, 76.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [71.7, 74.1, 74.7, 75.3, 76.1], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [73.4, 75.7, 76.7, 77.5, 78.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"title\": \"Comparison of classification f1-score for different algorithms (UB_SWR_HN)\", \"height\": 700, \"hovermode\": \"closest\", \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict7['500'][3]*100, nbdict7['1000'][3]*100, nbdict7['1500'][3]*100, nbdict7['2500'][3]*100, nbdict7['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict7['500'][3]*100, svmdict7['1000'][3]*100, svmdict7['1500'][3]*100, svmdict7['2500'][3]*100, svmdict7['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict7['500'][3]*100, lrdict7['1000'][3]*100, lrdict7['1500'][3]*100, lrdict7['2500'][3]*100, lrdict7['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict7['500'][3]*100, mlpdict7['1000'][3]*100, mlpdict7['1500'][3]*100, mlpdict7['2500'][3]*100, mlpdict7['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification f1-score for different algorithms (UB_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        range=[40,100]\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm by using a trained (on the same yelp data) word2vec vector model. The final resulting vector is the frequency weighted average of all word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480.6981029508379"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the reviews\n",
    "st = timer()\n",
    "tokenized_corpus = [word_tokenize(w) for w in df_final[\"text\"]]\n",
    "timer() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.34646401556165"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a word2vec model on the tokenized corpus\n",
    "st = timer()\n",
    "model = gensim.models.Word2Vec(tokenized_corpus, size=100, window=10, min_count=5, workers=12)\n",
    "timer() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 163.64 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate word vectors for each review and\n",
    "# get a weighted sum by frequency of words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "start_time = timer()\n",
    "word_vecs = []\n",
    "for i in range(len(df_final[\"text\"])):\n",
    "    # Filter out stop words or words not in the trained model\n",
    "    word = [w for w in tokenized_corpus[i] if w not in stop_words and w.isalpha() and w in model.wv.vocab]\n",
    "    C = Counter(word)\n",
    "    if len(word) == 0:\n",
    "        # If none of the words in the review are in the model\n",
    "        # the final vector is assigned as a zero vector\n",
    "        word_vecs.append(np.zeros((100,)))\n",
    "    else:\n",
    "        word_vecs.append(np.average([model.wv.get_vector(w) * C[w] for w in word], axis=0, weights=[C[w] for w in word]))\n",
    "        \n",
    "print(\"Finished in : {} seconds\\n\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We skip the naive bayes algorithm as it doesn't work with negative values\n",
    "def runClassifiersW2V(showRes = False):\n",
    "    \n",
    "    labels = df_final[\"rating\"].tolist()\n",
    "\n",
    "    trainX = np.asarray(word_vecs[0:70000] + word_vecs[100000:170000] + word_vecs[200000:270000])\n",
    "    testX = np.asarray(word_vecs[70000:100000] + word_vecs[170000:200000] + word_vecs[270000:])\n",
    "    trainY = labels[0:70000] + labels[100000:170000] + labels[200000:270000] \n",
    "    testY = labels[70000:100000] + labels[170000:200000] + labels[270000:]\n",
    "\n",
    "    # run linear SVM classifier\n",
    "    start_time = timer()\n",
    "    clf = SGDClassifier().fit(trainX, trainY)\n",
    "    train_time = round(timer()-start_time,2)\n",
    "    start_time = timer()\n",
    "    predYSVC = clf.predict(testX)\n",
    "    test_time = round(timer()-start_time,2)\n",
    "    accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "    precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "    recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "    f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "    svmlist = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "    if showRes:\n",
    "        test_y = pd.Series(testY, name='Actual')\n",
    "        pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "        df_confusion = pd.crosstab(test_y, pred_y)\n",
    "        print(\"\\n*************Linear SVM Classfier*************\")\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Macro averaged precision score: {}\".format(precision))\n",
    "        print(\"Macro averaged recall score: {}\".format(recall))\n",
    "        print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "        display(df_confusion)\n",
    "\n",
    "    # run logistic regression classifier\n",
    "    start_time = timer()\n",
    "    clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "    train_time = round(timer()-start_time,2)\n",
    "    start_time = timer()\n",
    "    predYLR = clf.predict(testX)\n",
    "    test_time = round(timer()-start_time,2)\n",
    "    accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "    precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "    recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "    f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "    lrlist = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "    if showRes:\n",
    "        test_y = pd.Series(testY, name='Actual')\n",
    "        pred_y = pd.Series(predYLR, name='Predicted')\n",
    "        df_confusion = pd.crosstab(test_y, pred_y)\n",
    "        print(\"\\n*************Logistic Regression Classfier*************\")\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Macro averaged precision score: {}\".format(precision))\n",
    "        print(\"Macro averaged recall score: {}\".format(recall))\n",
    "        print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "        display(df_confusion)\n",
    "\n",
    "    # transform the labels into one hot vector form\n",
    "    label_encoder = LabelEncoder()\n",
    "    trainYI = label_encoder.fit_transform(trainY)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "    trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "    testYI = label_encoder.fit_transform(testY)\n",
    "    testYI = testYI.reshape(len(testYI), 1)\n",
    "    testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=trainX.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, input_dim=50, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "    # train the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    start_time = timer()\n",
    "    model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "    train_time = round(timer()-start_time,2)\n",
    "\n",
    "    # Predict on the test using the trained model\n",
    "    start_time = timer()\n",
    "    pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "    test_time = round(timer()-start_time,2)\n",
    "    # process the predicted probailities to get the final labels encodedLabs\n",
    "    encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "    # flatten the list\n",
    "    predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "    accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "    precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "    recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "    f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "    mlplist = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "    if showRes:\n",
    "        # Print the performance metrics\n",
    "        print(\"\\n*************MLP Classfier*************\")\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Macro averaged precision score: {}\".format(precision))\n",
    "        print(\"Macro averaged recall score: {}\".format(recall))\n",
    "        print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "\n",
    "        # print the confusion matrix\n",
    "        test_y = pd.Series(testY, name='Actual')\n",
    "        pred_y = pd.Series(predLabs, name='Predicted')\n",
    "        df_confusion = pd.crosstab(test_y, pred_y)\n",
    "        display(df_confusion)\n",
    "            \n",
    "    return svmlist, lrlist, mlplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************Linear SVM Classfier*************\n",
      "Accuracy: 0.534\n",
      "Macro averaged precision score: 0.576\n",
      "Macro averaged recall score: 0.534\n",
      "Macro averaged f-1 score: 0.484\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>26833</td>\n",
       "      <td>1071</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>18762</td>\n",
       "      <td>4144</td>\n",
       "      <td>7094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>11277</td>\n",
       "      <td>1615</td>\n",
       "      <td>17108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  negative  neutral  positive\n",
       "Actual                                \n",
       "negative      26833     1071      2096\n",
       "neutral       18762     4144      7094\n",
       "positive      11277     1615     17108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************Logistic Regression Classfier*************\n",
      "Accuracy: 0.461\n",
      "Macro averaged precision score: 0.568\n",
      "Macro averaged recall score: 0.461\n",
      "Macro averaged f-1 score: 0.407\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>9710</td>\n",
       "      <td>1344</td>\n",
       "      <td>18946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>3244</td>\n",
       "      <td>4084</td>\n",
       "      <td>22672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1087</td>\n",
       "      <td>1235</td>\n",
       "      <td>27678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  negative  neutral  positive\n",
       "Actual                                \n",
       "negative       9710     1344     18946\n",
       "neutral        3244     4084     22672\n",
       "positive       1087     1235     27678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************MLP Classfier*************\n",
      "Accuracy: 0.674\n",
      "Macro averaged precision score: 0.679\n",
      "Macro averaged recall score: 0.674\n",
      "Macro averaged f-1 score: 0.673\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>23511</td>\n",
       "      <td>4787</td>\n",
       "      <td>1702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>8070</td>\n",
       "      <td>17615</td>\n",
       "      <td>4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>4207</td>\n",
       "      <td>6262</td>\n",
       "      <td>19531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  negative  neutral  positive\n",
       "Actual                                \n",
       "negative      23511     4787      1702\n",
       "neutral        8070    17615      4315\n",
       "positive       4207     6262     19531"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 50.43 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "st = timer()\n",
    "svmlist, lrlist, mlplist = runClassifiersW2V(True)\n",
    "print(\"Finished in : {} seconds\\n\".format(round(timer()-st,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.array(svmlist).dump(open('Results\\\\Word2Vec\\\\svmlist.npy', 'wb'))\n",
    "# np.array(lrlist).dump(open('Results\\\\Word2Vec\\\\lrlist.npy', 'wb'))\n",
    "# np.array(mlplist).dump(open('Results\\\\Word2Vec\\\\mlplist.npy', 'wb'))\n",
    "\n",
    "# Load the results\n",
    "svmlist = np.load(open('Results\\\\Word2Vec\\\\svmlist.npy', 'rb'))\n",
    "lrlist = np.load(open('Results\\\\Word2Vec\\\\lrlist.npy', 'rb'))\n",
    "mlplist = np.load(open('Results\\\\Word2Vec\\\\mlplist.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Linear SVM",
         "opacity": 0.8,
         "text": [
          52,
          58.4,
          52,
          48.7
         ],
         "textfont": {
          "color": "black",
          "family": "Calibri",
          "size": 14
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score"
         ],
         "y": [
          52,
          58.4,
          52,
          48.699999999999996
         ]
        },
        {
         "name": "Logistic Regression",
         "opacity": 0.8,
         "text": [
          46.6,
          56.4,
          46.6,
          42.5
         ],
         "textfont": {
          "color": "black",
          "family": "Calibri",
          "size": 14
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score"
         ],
         "y": [
          46.6,
          56.39999999999999,
          46.6,
          42.5
         ]
        },
        {
         "name": "Neural Net",
         "opacity": 0.8,
         "text": [
          68.3,
          68.9,
          68.3,
          68.5
         ],
         "textfont": {
          "color": "black",
          "family": "Calibri",
          "size": 14
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score"
         ],
         "y": [
          68.30000000000001,
          68.89999999999999,
          68.30000000000001,
          68.5
         ]
        }
       ],
       "layout": {
        "title": "Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)",
        "yaxis": {
         "title": "Percent value"
        }
       }
      },
      "text/html": [
       "<div id=\"2be1d813-a52f-4aef-b978-35f4377c24b1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2be1d813-a52f-4aef-b978-35f4377c24b1\", [{\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Linear SVM\", \"text\": [52.0, 58.4, 52.0, 48.7], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [52.0, 58.4, 52.0, 48.699999999999996], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Logistic Regression\", \"text\": [46.6, 56.4, 46.6, 42.5], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [46.6, 56.39999999999999, 46.6, 42.5], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Neural Net\", \"text\": [68.3, 68.9, 68.3, 68.5], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [68.30000000000001, 68.89999999999999, 68.30000000000001, 68.5], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}], {\"title\": \"Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)\", \"yaxis\": {\"title\": \"Percent value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"2be1d813-a52f-4aef-b978-35f4377c24b1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2be1d813-a52f-4aef-b978-35f4377c24b1\", [{\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Linear SVM\", \"text\": [52.0, 58.4, 52.0, 48.7], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [52.0, 58.4, 52.0, 48.699999999999996], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Logistic Regression\", \"text\": [46.6, 56.4, 46.6, 42.5], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [46.6, 56.39999999999999, 46.6, 42.5], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Neural Net\", \"text\": [68.3, 68.9, 68.3, 68.5], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [68.30000000000001, 68.89999999999999, 68.30000000000001, 68.5], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}], {\"title\": \"Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)\", \"yaxis\": {\"title\": \"Percent value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY PRECISION RECALL AND F1-SCORE\n",
    "# Create traces\n",
    "trace0 = Bar(\n",
    "    y = [svmlist[0]*100, svmlist[1]*100, svmlist[2]*100, svmlist[3]*100],\n",
    "    x = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "    text = [round(svmlist[0]*100,2), round(svmlist[1]*100,2), round(svmlist[2]*100,2), round(svmlist[3]*100,2)],\n",
    "    textfont=dict(family='Calibri', size=14, color='black'),\n",
    "    textposition = 'auto',\n",
    "    name = 'Linear SVM',\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "trace1 = Bar(\n",
    "    y = [lrlist[0]*100, lrlist[1]*100, lrlist[2]*100, lrlist[3]*100],\n",
    "    x = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "    text = [round(lrlist[0]*100,2), round(lrlist[1]*100,2), round(lrlist[2]*100,2), round(lrlist[3]*100,2)],\n",
    "    textposition = 'auto',\n",
    "    textfont=dict(family='Calibri', size=14, color='black'),\n",
    "    name = 'Logistic Regression',\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "trace2 = Bar(\n",
    "    x = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "    y = [mlplist[0]*100, mlplist[1]*100, mlplist[2]*100, mlplist[3]*100],\n",
    "    text = [round(mlplist[0]*100,2), round(mlplist[1]*100,2), round(mlplist[2]*100,2), round(mlplist[3]*100,2)],\n",
    "    textfont=dict(family='Calibri', size=14, color='black'),\n",
    "    textposition = 'auto',\n",
    "    name = 'Neural Net',\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2]\n",
    "layout = Layout(title=\"Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)\", \n",
    "               yaxis=dict(title=\"Percent value\"))\n",
    "fig = Figure(data=data,layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification into two classes - 'positive' and 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams (U), stop words removed (SWR), punctuations removed (PR) (tfidfvectorizer default setting) and vary the feature size in 500, 1000, 1500, 2500 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_posneg = df_final.loc[(df_final[\"rating\"] == 'negative') | (df_final[\"rating\"] == 'positive')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that runs the classifiers on unigram features with stop words and punctuations removed\n",
    "def runClassifiersU2C(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        print(\"\\nRunning classifiers for feature size : {}\".format(MaxFeatures))\n",
    "        # Initialize the tfidf vectorizer \n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = MaxFeatures, norm='l2')\n",
    "        # Run the vectorizer on the yelp reviews\n",
    "        tokens = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        # Split into training and test set 70:30 split\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] \n",
    "        testY = labels[70000:100000] + labels[170000:]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "        # if showRes is true the confusion matrix for each run is printed\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logistic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running classifiers for feature size : 500\n",
      "\n",
      "Running classifiers for feature size : 1000\n",
      "\n",
      "Running classifiers for feature size : 1500\n",
      "\n",
      "Running classifiers for feature size : 2500\n",
      "\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 438.31 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict, svmdict, lrdict, mlpdict = runClassifiersU2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "## Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR\\\\nbdict.npy\", nbdict)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR\\\\svmdict.npy\", svmdict)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR\\\\lrdict.npy\", lrdict)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR\\\\mlpdict.npy\", mlpdict)\n",
    "\n",
    "## to load results from file\n",
    "nbdict = np.load(\"Results\\\\Two_class\\\\U_SWR_PR\\\\nbdict.npy\").item()\n",
    "svmdict = np.load(\"Results\\\\Two_class\\\\U_SWR_PR\\\\svmdict.npy\").item()\n",
    "lrdict = np.load(\"Results\\\\Two_class\\\\U_SWR_PR\\\\lrdict.npy\").item()\n",
    "mlpdict = np.load(\"Results\\\\Two_class\\\\U_SWR_PR\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy, training time, testing time and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          86.7,
          88,
          88.2,
          88.6,
          89
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.5,
          91.2,
          91.9,
          92.60000000000001,
          93
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.2,
          90.9,
          91.5,
          91.9,
          92.10000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.2,
          91.9,
          92.5,
          93.10000000000001,
          93.30000000000001
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_U_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"a7d93dbf-a662-4966-bce4-80a2a1512a12\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"a7d93dbf-a662-4966-bce4-80a2a1512a12\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.7, 88.0, 88.2, 88.6, 89.0], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.5, 91.2, 91.9, 92.60000000000001, 93.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.2, 90.9, 91.5, 91.9, 92.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.2, 91.9, 92.5, 93.10000000000001, 93.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"a7d93dbf-a662-4966-bce4-80a2a1512a12\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"a7d93dbf-a662-4966-bce4-80a2a1512a12\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.7, 88.0, 88.2, 88.6, 89.0], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.5, 91.2, 91.9, 92.60000000000001, 93.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.2, 90.9, 91.5, 91.9, 92.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.2, 91.9, 92.5, 93.10000000000001, 93.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][0]*100, nbdict['1000'][0]*100, nbdict['1500'][0]*100, nbdict['2500'][0]*100, nbdict['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][0]*100, svmdict['1000'][0]*100, svmdict['1500'][0]*100, svmdict['2500'][0]*100, svmdict['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][0]*100, lrdict['1000'][0]*100, lrdict['1500'][0]*100, lrdict['2500'][0]*100, lrdict['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][0]*100, mlpdict['1000'][0]*100, mlpdict['1500'][0]*100, mlpdict['2500'][0]*100, mlpdict['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_U_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.18,
          0.17,
          0.17,
          0.18,
          0.18
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.25,
          0.27,
          0.28,
          0.29,
          0.3
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.33,
          0.35,
          0.36,
          0.36,
          0.38
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          50.28,
          56.35,
          60.46,
          66.95,
          76.94
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of training time for different algorithms (2C_U_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Training time in seconds (log-scale)",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div id=\"6297b6ad-86e0-4eeb-952f-66ab19df80ef\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6297b6ad-86e0-4eeb-952f-66ab19df80ef\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.18, 0.17, 0.17, 0.18, 0.18], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [0.25, 0.27, 0.28, 0.29, 0.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [0.33, 0.35, 0.36, 0.36, 0.38], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [50.28, 56.35, 60.46, 66.95, 76.94], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"type\": \"log\", \"gridwidth\": 2, \"title\": \"Training time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of training time for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6297b6ad-86e0-4eeb-952f-66ab19df80ef\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6297b6ad-86e0-4eeb-952f-66ab19df80ef\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.18, 0.17, 0.17, 0.18, 0.18], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [0.25, 0.27, 0.28, 0.29, 0.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [0.33, 0.35, 0.36, 0.36, 0.38], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [50.28, 56.35, 60.46, 66.95, 76.94], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"type\": \"log\", \"gridwidth\": 2, \"title\": \"Training time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of training time for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE TRAINING TIME\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][4], nbdict['1000'][4], nbdict['1500'][4], nbdict['2500'][4], nbdict['5000'][4]],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    legendgroup=\"1\",\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    "    #5799C7\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][4], svmdict['1000'][4], svmdict['1500'][4], svmdict['2500'][4], svmdict['5000'][4]],\n",
    "    legendgroup=\"2\",\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM',\n",
    "    marker= {'color': 'rgb(255, 127, 14)'}\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][4], lrdict['1000'][4], lrdict['1500'][4], lrdict['2500'][4], lrdict['5000'][4]],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"3\",\n",
    "    name = 'Logistic Regression',\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][4], mlpdict['1000'][4], mlpdict['1500'][4], mlpdict['2500'][4], mlpdict['5000'][4]],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"4\",\n",
    "    name = 'Neural net',\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of training time for different algorithms (2C_U_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Training time in seconds (log-scale)',\n",
    "        type='log',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.01,
          0.01,
          0.01,
          0.01,
          0.01
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.01,
          0.01,
          0.01,
          0.01,
          0.01
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.01,
          0.01,
          0.01,
          0.01,
          0.01
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          0.52,
          0.6,
          0.65,
          0.74,
          0.82
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of testing time for different algorithms (2C_U_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Testing time in seconds (log-scale)",
         "type": "log"
        }
       }
      },
      "text/html": [
       "<div id=\"8dfd70d4-5f15-45eb-9f3e-0135c17ceeb5\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8dfd70d4-5f15-45eb-9f3e-0135c17ceeb5\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.01, 0.01, 0.01, 0.01, 0.01], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [0.01, 0.01, 0.01, 0.01, 0.01], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [0.01, 0.01, 0.01, 0.01, 0.01], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [0.52, 0.6, 0.65, 0.74, 0.82], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"type\": \"log\", \"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Testing time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of testing time for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8dfd70d4-5f15-45eb-9f3e-0135c17ceeb5\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8dfd70d4-5f15-45eb-9f3e-0135c17ceeb5\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [0.01, 0.01, 0.01, 0.01, 0.01], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [0.01, 0.01, 0.01, 0.01, 0.01], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [0.01, 0.01, 0.01, 0.01, 0.01], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [0.52, 0.6, 0.65, 0.74, 0.82], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"type\": \"log\", \"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Testing time in seconds (log-scale)\"}, \"hovermode\": \"closest\", \"title\": \"Comparison of testing time for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE TESTING TIME\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][5], nbdict['1000'][5], nbdict['1500'][5], nbdict['2500'][5], nbdict['5000'][5]],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][5], svmdict['1000'][5], svmdict['1500'][5], svmdict['2500'][5], svmdict['5000'][5]],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][5], lrdict['1000'][5], lrdict['1500'][5], lrdict['2500'][5], lrdict['5000'][5]],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][5], mlpdict['1000'][5], mlpdict['1500'][5], mlpdict['2500'][5], mlpdict['5000'][5]],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of testing time for different algorithms (2C_U_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Testing time in seconds (log-scale)',\n",
    "        ticklen= 5,\n",
    "        gridwidth= 2,\n",
    "        type='log'\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          86.7,
          88,
          88.2,
          88.6,
          89
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.5,
          91.2,
          91.9,
          92.60000000000001,
          93
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.2,
          90.9,
          91.5,
          91.9,
          92.10000000000001
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.2,
          91.9,
          92.5,
          93.10000000000001,
          93.30000000000001
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_U_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"d65f6e51-b1cd-4d14-b738-df39f4b6899c\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d65f6e51-b1cd-4d14-b738-df39f4b6899c\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.7, 88.0, 88.2, 88.6, 89.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [89.5, 91.2, 91.9, 92.60000000000001, 93.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [89.2, 90.9, 91.5, 91.9, 92.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [90.2, 91.9, 92.5, 93.10000000000001, 93.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d65f6e51-b1cd-4d14-b738-df39f4b6899c\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d65f6e51-b1cd-4d14-b738-df39f4b6899c\", [{\"type\": \"scatter\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.7, 88.0, 88.2, 88.6, 89.0], \"name\": \"Naive Bayes\"}, {\"type\": \"scatter\", \"y\": [89.5, 91.2, 91.9, 92.60000000000001, 93.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Linear SVM\"}, {\"type\": \"scatter\", \"y\": [89.2, 90.9, 91.5, 91.9, 92.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Logistic Regression\"}, {\"type\": \"scatter\", \"y\": [90.2, 91.9, 92.5, 93.10000000000001, 93.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"name\": \"Neural net\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_U_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-SCORE\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict['500'][2]*100, nbdict['1000'][3]*100, nbdict['1500'][3]*100, nbdict['2500'][3]*100, nbdict['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Naive Bayes'\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict['500'][3]*100, svmdict['1000'][3]*100, svmdict['1500'][3]*100, svmdict['2500'][3]*100, svmdict['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict['500'][3]*100, lrdict['1000'][3]*100, lrdict['1500'][3]*100, lrdict['2500'][3]*100, lrdict['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression'\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict['500'][3]*100, mlpdict['1000'][3]*100, mlpdict['1500'][3]*100, mlpdict['2500'][3]*100, mlpdict['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net'\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_U_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams, stop words removed, punctuations removed (tfidfvectorizer default setting), setting the feature size to 5000 and varying the minimum document frequency threshold in 5,10,15 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiers_df2C(showRes = False):\n",
    "    df_array = [5,10,15,20]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for mdf in df_array:\n",
    "        print(\"Running classifiers with min document frequency threshold : {}\".format(mdf))\n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = 5000, norm='l2', min_df=mdf)\n",
    "        tokens = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(mdf)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers with min document frequency threshold : 5\n",
      "Running classifiers with min document frequency threshold : 10\n",
      "Running classifiers with min document frequency threshold : 15\n",
      "Running classifiers with min document frequency threshold : 20\n",
      "Finished in : 409.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict2, svmdict2, lrdict2, mlpdict2 = runClassifiers_df2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\nbdict.npy\", nbdict2)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\svmdict.npy\", svmdict2)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\lrdict.npy\", lrdict2)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\mlpdict.npy\", mlpdict2)\n",
    "\n",
    "## to load results from file\n",
    "nbdict2 = np.load(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\nbdict.npy\").item()\n",
    "svmdict2 = np.load(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\svmdict.npy\").item()\n",
    "lrdict2 = np.load(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\lrdict.npy\").item()\n",
    "mlpdict2 = np.load(\"Results\\\\Two_class\\\\U_SWR_PR_DF\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          89,
          89,
          89,
          89
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          93,
          92.9,
          92.9,
          93
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          92.2,
          92.10000000000001,
          92.2,
          92.2
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          93.5,
          93.4,
          93.4,
          93.5
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_U_SWR_PR_DF)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Minimum document frequency",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"bc438313-4310-4222-80dd-421d9ea693fa\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"bc438313-4310-4222-80dd-421d9ea693fa\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [89.0, 89.0, 89.0, 89.0], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [93.0, 92.9, 92.9, 93.0], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [92.2, 92.10000000000001, 92.2, 92.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [93.5, 93.4, 93.4, 93.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"bc438313-4310-4222-80dd-421d9ea693fa\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"bc438313-4310-4222-80dd-421d9ea693fa\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [89.0, 89.0, 89.0, 89.0], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [93.0, 92.9, 92.9, 93.0], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [92.2, 92.10000000000001, 92.2, 92.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [93.5, 93.4, 93.4, 93.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict2['5'][0]*100, nbdict2['10'][0]*100, nbdict2['15'][0]*100, nbdict2['20'][0]*100],\n",
    "    x = [5,10,15,20],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [svmdict2['5'][0]*100, svmdict2['10'][0]*100, svmdict2['15'][0]*100, svmdict2['20'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [lrdict2['5'][0]*100, lrdict2['10'][0]*100, lrdict2['15'][0]*100, lrdict2['20'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [mlpdict2['5'][0]*100, mlpdict2['10'][0]*100, mlpdict2['15'][0]*100, mlpdict2['20'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_U_SWR_PR_DF)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Minimum document frequency',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          89,
          89,
          89,
          89
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          93,
          92.9,
          92.9,
          93
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          92.2,
          92.10000000000001,
          92.2,
          92.2
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          5,
          10,
          15,
          20
         ],
         "y": [
          93.5,
          93.4,
          93.4,
          93.5
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_U_SWR_PR_DF)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Minimum document frequency",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"6906218c-43c2-4fc1-836a-1d70ca2345a5\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6906218c-43c2-4fc1-836a-1d70ca2345a5\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [89.0, 89.0, 89.0, 89.0], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [93.0, 92.9, 92.9, 93.0], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [92.2, 92.10000000000001, 92.2, 92.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [93.5, 93.4, 93.4, 93.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6906218c-43c2-4fc1-836a-1d70ca2345a5\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6906218c-43c2-4fc1-836a-1d70ca2345a5\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [5, 10, 15, 20], \"mode\": \"lines+markers\", \"y\": [89.0, 89.0, 89.0, 89.0], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [93.0, 92.9, 92.9, 93.0], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [92.2, 92.10000000000001, 92.2, 92.2], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [93.5, 93.4, 93.4, 93.5], \"mode\": \"lines+markers\", \"x\": [5, 10, 15, 20], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_U_SWR_PR_DF)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Minimum document frequency\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict2['5'][3]*100, nbdict2['10'][3]*100, nbdict2['15'][3]*100, nbdict2['20'][3]*100],\n",
    "    x = [5,10,15,20],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [svmdict2['5'][3]*100, svmdict2['10'][3]*100, svmdict2['15'][3]*100, svmdict2['20'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [lrdict2['5'][3]*100, lrdict2['10'][3]*100, lrdict2['15'][3]*100, lrdict2['20'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [5,10,15,20],\n",
    "    y = [mlpdict2['5'][3]*100, mlpdict2['10'][3]*100, mlpdict2['15'][3]*100, mlpdict2['20'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_U_SWR_PR_DF)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Minimum document frequency',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams (U), stop words removed(SWR) and vary the feature size in 500, 1000, 1500, 2500 and 5000, while also handling negations(HN).\n",
    "\n",
    "**NOTE**: The mark negation function takes a about 10 mins to run for each feature size. For 5 feature sizes it takes around 50 mins to get the tfidf vectors with negations marked and another 10 mins to run the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 377.52 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 500, norm='l2')\n",
    "tokens500 = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 371.31 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1000, norm='l2')\n",
    "tokens1000 = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 371.96 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1500, norm='l2')\n",
    "tokens1500 = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 372.16 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 2500, norm='l2')\n",
    "tokens2500 = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 374.07 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 5000, norm='l2')\n",
    "tokens5000 = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersUMN2C(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        if MaxFeatures == 500:\n",
    "            tokens = tokens500\n",
    "        elif MaxFeatures == 1000:\n",
    "            tokens = tokens1000\n",
    "        elif MaxFeatures == 1500:\n",
    "            tokens = tokens1500\n",
    "        elif MaxFeatures == 2500:\n",
    "            tokens = tokens2500\n",
    "        elif MaxFeatures == 5000:\n",
    "            tokens = tokens5000\n",
    "        \n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "                \n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 379.38 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict3, svmdict3, lrdict3, mlpdict3 = runClassifiersUMN2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "## Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_HN\\\\nbdict.npy\", nbdict3)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_HN\\\\svmdict.npy\", svmdict3)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_HN\\\\lrdict.npy\", lrdict3)\n",
    "# np.save(\"Results\\\\Two_class\\\\U_SWR_HN\\\\mlpdict.npy\", mlpdict3)\n",
    "\n",
    "## to load results from file\n",
    "nbdict3 = np.load(\"Results\\\\Two_class\\\\U_SWR_HN\\\\nbdict.npy\").item()\n",
    "svmdict3 = np.load(\"Results\\\\Two_class\\\\U_SWR_HN\\\\svmdict.npy\").item()\n",
    "lrdict3 = np.load(\"Results\\\\Two_class\\\\U_SWR_HN\\\\lrdict.npy\").item()\n",
    "mlpdict3 = np.load(\"Results\\\\Two_class\\\\U_SWR_HN\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          85.39999999999999,
          87,
          87.6,
          88.3,
          88.9
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.4,
          92.10000000000001,
          92.80000000000001,
          93.30000000000001,
          93.7
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90,
          91.5,
          92,
          92.4,
          92.60000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          91.3,
          92.9,
          93.4,
          94,
          94.19999999999999
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_U_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"c64f924a-719e-42f3-9e60-f29d1bb3a7be\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c64f924a-719e-42f3-9e60-f29d1bb3a7be\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [85.39999999999999, 87.0, 87.6, 88.3, 88.9], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [90.4, 92.10000000000001, 92.80000000000001, 93.30000000000001, 93.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [90.0, 91.5, 92.0, 92.4, 92.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [91.3, 92.9, 93.4, 94.0, 94.19999999999999], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_U_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c64f924a-719e-42f3-9e60-f29d1bb3a7be\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c64f924a-719e-42f3-9e60-f29d1bb3a7be\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [85.39999999999999, 87.0, 87.6, 88.3, 88.9], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [90.4, 92.10000000000001, 92.80000000000001, 93.30000000000001, 93.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [90.0, 91.5, 92.0, 92.4, 92.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [91.3, 92.9, 93.4, 94.0, 94.19999999999999], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_U_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict3['500'][0]*100, nbdict3['1000'][0]*100, nbdict3['1500'][0]*100, nbdict3['2500'][0]*100, nbdict3['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict3['500'][0]*100, svmdict3['1000'][0]*100, svmdict3['1500'][0]*100, svmdict3['2500'][0]*100, svmdict3['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict3['500'][0]*100, lrdict3['1000'][0]*100, lrdict3['1500'][0]*100, lrdict3['2500'][0]*100, lrdict3['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict3['500'][0]*100, mlpdict3['1000'][0]*100, mlpdict3['1500'][0]*100, mlpdict3['2500'][0]*100, mlpdict3['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_U_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          85.39999999999999,
          86.9,
          87.6,
          88.2,
          88.9
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.4,
          92.10000000000001,
          92.80000000000001,
          93.30000000000001,
          93.7
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90,
          91.5,
          92,
          92.4,
          92.60000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          91.3,
          92.9,
          93.4,
          94,
          94.19999999999999
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_U_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"adb9f23e-3765-4e3f-b3e0-ddf443e3a6a4\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"adb9f23e-3765-4e3f-b3e0-ddf443e3a6a4\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [85.39999999999999, 86.9, 87.6, 88.2, 88.9], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [90.4, 92.10000000000001, 92.80000000000001, 93.30000000000001, 93.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [90.0, 91.5, 92.0, 92.4, 92.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [91.3, 92.9, 93.4, 94.0, 94.19999999999999], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_U_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"adb9f23e-3765-4e3f-b3e0-ddf443e3a6a4\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"adb9f23e-3765-4e3f-b3e0-ddf443e3a6a4\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [85.39999999999999, 86.9, 87.6, 88.2, 88.9], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [90.4, 92.10000000000001, 92.80000000000001, 93.30000000000001, 93.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [90.0, 91.5, 92.0, 92.4, 92.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [91.3, 92.9, 93.4, 94.0, 94.19999999999999], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_U_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict3['500'][3]*100, nbdict3['1000'][3]*100, nbdict3['1500'][3]*100, nbdict3['2500'][3]*100, nbdict3['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict3['500'][3]*100, svmdict3['1000'][3]*100, svmdict3['1500'][3]*100, svmdict3['2500'][3]*100, svmdict3['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict3['500'][3]*100, lrdict3['1000'][3]*100, lrdict3['1500'][3]*100, lrdict3['2500'][3]*100, lrdict3['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict3['500'][3]*100, mlpdict3['1000'][3]*100, mlpdict3['1500'][3]*100, mlpdict3['2500'][3]*100, mlpdict3['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_U_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with bigrams(only) (B), stop words removed (SWR), punctuations removed (tfidfvectorizer default setting) (PR) and vary the feature size in 500, 1000, 1500, 2500 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersB2C(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = MaxFeatures, ngram_range=(2, 2), norm='l2')\n",
    "        tokens = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logistic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 479.17 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict4, svmdict4, lrdict4, mlpdict4 = runClassifiersB2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "## Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_PR\\\\nbdict.npy\", nbdict4)\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_PR\\\\svmdict.npy\", svmdict4)\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_PR\\\\lrdict.npy\", lrdict4)\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_PR\\\\mlpdict.npy\", mlpdict4)\n",
    "\n",
    "## to load results from file\n",
    "nbdict4 = np.load(\"Results\\\\Two_class\\\\B_SWR_PR\\\\nbdict.npy\").item()\n",
    "svmdict4 = np.load(\"Results\\\\Two_class\\\\B_SWR_PR\\\\svmdict.npy\").item()\n",
    "lrdict4 = np.load(\"Results\\\\Two_class\\\\B_SWR_PR\\\\lrdict.npy\").item()\n",
    "mlpdict4 = np.load(\"Results\\\\Two_class\\\\B_SWR_PR\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          75.2,
          78.4,
          80.2,
          82.39999999999999,
          84.89999999999999
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          75,
          78.2,
          80.30000000000001,
          82.5,
          85
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          75.1,
          78.3,
          80.2,
          82.3,
          84.5
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          75.3,
          78.5,
          80.5,
          82.89999999999999,
          85.3
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_B_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"5311bd47-7808-4b00-80f1-c69781519864\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5311bd47-7808-4b00-80f1-c69781519864\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [75.2, 78.4, 80.2, 82.39999999999999, 84.89999999999999], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [75.0, 78.2, 80.30000000000001, 82.5, 85.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [75.1, 78.3, 80.2, 82.3, 84.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [75.3, 78.5, 80.5, 82.89999999999999, 85.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_B_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5311bd47-7808-4b00-80f1-c69781519864\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5311bd47-7808-4b00-80f1-c69781519864\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [75.2, 78.4, 80.2, 82.39999999999999, 84.89999999999999], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [75.0, 78.2, 80.30000000000001, 82.5, 85.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [75.1, 78.3, 80.2, 82.3, 84.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [75.3, 78.5, 80.5, 82.89999999999999, 85.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_B_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict4['500'][0]*100, nbdict4['1000'][0]*100, nbdict4['1500'][0]*100, nbdict4['2500'][0]*100, nbdict4['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict4['500'][0]*100, svmdict4['1000'][0]*100, svmdict4['1500'][0]*100, svmdict4['2500'][0]*100, svmdict4['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict4['500'][0]*100, lrdict4['1000'][0]*100, lrdict4['1500'][0]*100, lrdict4['2500'][0]*100, lrdict4['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict4['500'][0]*100, mlpdict4['1000'][0]*100, mlpdict4['1500'][0]*100, mlpdict4['2500'][0]*100, mlpdict4['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_B_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          75,
          78.2,
          80.2,
          82.3,
          84.89999999999999
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          74.7,
          78.2,
          80.2,
          82.5,
          85
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          74.9,
          78.3,
          80.10000000000001,
          82.3,
          84.5
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          75,
          78.4,
          80.4,
          82.8,
          85.3
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_B_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"150d4106-c243-4067-9e0f-f4fbd867cc0f\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"150d4106-c243-4067-9e0f-f4fbd867cc0f\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [75.0, 78.2, 80.2, 82.3, 84.89999999999999], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [74.7, 78.2, 80.2, 82.5, 85.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [74.9, 78.3, 80.10000000000001, 82.3, 84.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [75.0, 78.4, 80.4, 82.8, 85.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_B_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"150d4106-c243-4067-9e0f-f4fbd867cc0f\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"150d4106-c243-4067-9e0f-f4fbd867cc0f\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [75.0, 78.2, 80.2, 82.3, 84.89999999999999], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [74.7, 78.2, 80.2, 82.5, 85.0], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [74.9, 78.3, 80.10000000000001, 82.3, 84.5], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [75.0, 78.4, 80.4, 82.8, 85.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_B_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict4['500'][3]*100, nbdict4['1000'][3]*100, nbdict4['1500'][3]*100, nbdict4['2500'][3]*100, nbdict4['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict4['500'][3]*100, svmdict4['1000'][3]*100, svmdict4['1500'][3]*100, svmdict4['2500'][3]*100, svmdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict4['500'][3]*100, lrdict4['1000'][3]*100, lrdict4['1500'][3]*100, lrdict4['2500'][3]*100, lrdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict4['500'][3]*100, mlpdict4['1000'][3]*100, mlpdict4['1500'][3]*100, mlpdict4['2500'][3]*100, mlpdict4['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_B_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with bigrams(only) (B), stop words removed (SWR) and vary the feature size in 500, 1000, 1500, 2500 and 5000, while handling negations (HN).\n",
    "\n",
    "**NOTE**: The mark negation function takes a about 10 mins to run for each feature size. For 5 feature sizes it takes around 50 mins to get the tfidf vectors with negations marked and another 10 mins to run the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 416.23 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 500, ngram_range=(2, 2), norm='l2')\n",
    "tokens500B = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 415.9 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1000, ngram_range=(2, 2), norm='l2')\n",
    "tokens1000B = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 414.8 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1500, ngram_range=(2, 2), norm='l2')\n",
    "tokens1500B = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 405.35 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 2500, ngram_range=(2, 2), norm='l2')\n",
    "tokens2500B = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 404.84 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 5000, ngram_range=(2, 2), norm='l2')\n",
    "tokens5000B = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersBN2C(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        if MaxFeatures == 500:\n",
    "            tokens = tokens500B\n",
    "        elif MaxFeatures == 1000:\n",
    "            tokens = tokens1000B\n",
    "        elif MaxFeatures == 1500:\n",
    "            tokens = tokens1500B\n",
    "        elif MaxFeatures == 2500:\n",
    "            tokens = tokens2500B\n",
    "        elif MaxFeatures == 5000:\n",
    "            tokens = tokens5000B\n",
    "            \n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "                \n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000]\n",
    "        testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 274.84 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict5, svmdict5, lrdict5, mlpdict5 = runClassifiersBN2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_HN\\\\nbdict.npy\", nbdict5)\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_HN\\\\svmdict.npy\", svmdict5)\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_HN\\\\lrdict.npy\", lrdict5)\n",
    "# np.save(\"Results\\\\Two_class\\\\B_SWR_HN\\\\mlpdict.npy\", mlpdict5)\n",
    "\n",
    "## to load results from file\n",
    "nbdict5 = np.load(\"Results\\\\Two_class\\\\B_SWR_HN\\\\nbdict.npy\").item()\n",
    "svmdict5 = np.load(\"Results\\\\Two_class\\\\B_SWR_HN\\\\svmdict.npy\").item()\n",
    "lrdict5 = np.load(\"Results\\\\Two_class\\\\B_SWR_HN\\\\lrdict.npy\").item()\n",
    "mlpdict5 = np.load(\"Results\\\\Two_class\\\\B_SWR_HN\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          80.7,
          83.7,
          85.8,
          87.5,
          89.2
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          82.19999999999999,
          85.3,
          87.4,
          89,
          90.7
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          82.19999999999999,
          85.1,
          87.2,
          88.7,
          90.10000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          82.69999999999999,
          85.7,
          87.7,
          89.60000000000001,
          91.10000000000001
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_B_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"aa7c17f7-1885-4ded-8235-6c4ac8946871\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"aa7c17f7-1885-4ded-8235-6c4ac8946871\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [80.7, 83.7, 85.8, 87.5, 89.2], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [82.19999999999999, 85.3, 87.4, 89.0, 90.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [82.19999999999999, 85.1, 87.2, 88.7, 90.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [82.69999999999999, 85.7, 87.7, 89.60000000000001, 91.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_B_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"aa7c17f7-1885-4ded-8235-6c4ac8946871\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"aa7c17f7-1885-4ded-8235-6c4ac8946871\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [80.7, 83.7, 85.8, 87.5, 89.2], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [82.19999999999999, 85.3, 87.4, 89.0, 90.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [82.19999999999999, 85.1, 87.2, 88.7, 90.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [82.69999999999999, 85.7, 87.7, 89.60000000000001, 91.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_B_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict5['500'][0]*100, nbdict5['1000'][0]*100, nbdict5['1500'][0]*100, nbdict5['2500'][0]*100, nbdict5['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict5['500'][0]*100, svmdict5['1000'][0]*100, svmdict5['1500'][0]*100, svmdict5['2500'][0]*100, svmdict5['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict5['500'][0]*100, lrdict5['1000'][0]*100, lrdict5['1500'][0]*100, lrdict5['2500'][0]*100, lrdict5['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict5['500'][0]*100, mlpdict5['1000'][0]*100, mlpdict5['1500'][0]*100, mlpdict5['2500'][0]*100, mlpdict5['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_B_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          80.7,
          83.7,
          85.8,
          87.5,
          89.2
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          82.19999999999999,
          85.3,
          87.4,
          89,
          90.7
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          82.19999999999999,
          85.1,
          87.2,
          88.7,
          90.10000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          82.69999999999999,
          85.7,
          87.7,
          89.60000000000001,
          91.10000000000001
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_B_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"9dcfa981-78da-448c-b0f2-65984c1c47e7\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9dcfa981-78da-448c-b0f2-65984c1c47e7\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [80.7, 83.7, 85.8, 87.5, 89.2], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [82.19999999999999, 85.3, 87.4, 89.0, 90.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [82.19999999999999, 85.1, 87.2, 88.7, 90.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [82.69999999999999, 85.7, 87.7, 89.60000000000001, 91.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_B_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9dcfa981-78da-448c-b0f2-65984c1c47e7\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9dcfa981-78da-448c-b0f2-65984c1c47e7\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [80.7, 83.7, 85.8, 87.5, 89.2], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [82.19999999999999, 85.3, 87.4, 89.0, 90.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [82.19999999999999, 85.1, 87.2, 88.7, 90.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [82.69999999999999, 85.7, 87.7, 89.60000000000001, 91.10000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_B_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict5['500'][3]*100, nbdict5['1000'][3]*100, nbdict5['1500'][3]*100, nbdict5['2500'][3]*100, nbdict5['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict5['500'][3]*100, svmdict5['1000'][3]*100, svmdict5['1500'][3]*100, svmdict5['2500'][3]*100, svmdict5['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict5['500'][3]*100, lrdict5['1000'][3]*100, lrdict5['1500'][3]*100, lrdict5['2500'][3]*100, lrdict5['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict5['500'][3]*100, mlpdict5['1000'][3]*100, mlpdict5['1500'][3]*100, mlpdict5['2500'][3]*100, mlpdict5['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_B_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams+bigrams (UB), stop words removed (SWR), punctuations removed (tfidfvectorizer default setting) (PR) and vary the feature size in 500, 1000, 1500, 2500 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersUB2C(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "        tfidf_vect = TfidfVectorizer(stop_words=\"english\", max_features = MaxFeatures, ngram_range=(1, 2), norm='l2')\n",
    "        tokens = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 671.42 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict6, svmdict6, lrdict6, mlpdict6 = runClassifiersUB2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\nbdict.npy\", nbdict6)\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\svmdict.npy\", svmdict6)\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\lrdict.npy\", lrdict6)\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\mlpdict.npy\", mlpdict6)\n",
    "\n",
    "## to load results from file\n",
    "nbdict6 = np.load(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\nbdict.npy\").item()\n",
    "svmdict6 = np.load(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\svmdict.npy\").item()\n",
    "lrdict6 = np.load(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\lrdict.npy\").item()\n",
    "mlpdict6 = np.load(\"Results\\\\Two_class\\\\UB_SWR_PR\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          86.8,
          88,
          88.4,
          89,
          89.8
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.5,
          91.3,
          91.9,
          92.7,
          93.2
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.3,
          90.9,
          91.5,
          92,
          92.30000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.10000000000001,
          91.9,
          92.60000000000001,
          93.2,
          93.60000000000001
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_UB_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"19eddfa2-fa1a-4bf5-bf6f-6352cb655619\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"19eddfa2-fa1a-4bf5-bf6f-6352cb655619\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.8, 88.0, 88.4, 89.0, 89.8], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.5, 91.3, 91.9, 92.7, 93.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.3, 90.9, 91.5, 92.0, 92.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.10000000000001, 91.9, 92.60000000000001, 93.2, 93.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_UB_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"19eddfa2-fa1a-4bf5-bf6f-6352cb655619\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"19eddfa2-fa1a-4bf5-bf6f-6352cb655619\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.8, 88.0, 88.4, 89.0, 89.8], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.5, 91.3, 91.9, 92.7, 93.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.3, 90.9, 91.5, 92.0, 92.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.10000000000001, 91.9, 92.60000000000001, 93.2, 93.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_UB_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict6['500'][0]*100, nbdict6['1000'][0]*100, nbdict6['1500'][0]*100, nbdict6['2500'][0]*100, nbdict6['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict6['500'][0]*100, svmdict6['1000'][0]*100, svmdict6['1500'][0]*100, svmdict6['2500'][0]*100, svmdict6['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict6['500'][0]*100, lrdict6['1000'][0]*100, lrdict6['1500'][0]*100, lrdict6['2500'][0]*100, lrdict6['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict6['500'][0]*100, mlpdict6['1000'][0]*100, mlpdict6['1500'][0]*100, mlpdict6['2500'][0]*100, mlpdict6['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_UB_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          86.8,
          88,
          88.4,
          89,
          89.8
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.5,
          91.3,
          91.9,
          92.7,
          93.2
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.3,
          90.9,
          91.5,
          92,
          92.30000000000001
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.10000000000001,
          91.9,
          92.60000000000001,
          93.2,
          93.60000000000001
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_UB_SWR_PR)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"6bd5e7d9-af3c-4486-aedf-004bae590b85\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6bd5e7d9-af3c-4486-aedf-004bae590b85\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.8, 88.0, 88.4, 89.0, 89.8], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.5, 91.3, 91.9, 92.7, 93.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.3, 90.9, 91.5, 92.0, 92.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.10000000000001, 91.9, 92.60000000000001, 93.2, 93.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_UB_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6bd5e7d9-af3c-4486-aedf-004bae590b85\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6bd5e7d9-af3c-4486-aedf-004bae590b85\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [86.8, 88.0, 88.4, 89.0, 89.8], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.5, 91.3, 91.9, 92.7, 93.2], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.3, 90.9, 91.5, 92.0, 92.30000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.10000000000001, 91.9, 92.60000000000001, 93.2, 93.60000000000001], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_UB_SWR_PR)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict6['500'][3]*100, nbdict6['1000'][3]*100, nbdict6['1500'][3]*100, nbdict6['2500'][3]*100, nbdict6['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict6['500'][3]*100, svmdict6['1000'][3]*100, svmdict6['1500'][3]*100, svmdict6['2500'][3]*100, svmdict6['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict6['500'][3]*100, lrdict6['1000'][3]*100, lrdict6['1500'][3]*100, lrdict6['2500'][3]*100, lrdict6['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict6['500'][3]*100, mlpdict6['1000'][3]*100, mlpdict6['1500'][3]*100, mlpdict6['2500'][3]*100, mlpdict6['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_UB_SWR_PR)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm on using tfidf vectors with unigrams+bigrams (UB), stop words removed (SWR) and vary the feature size in 500, 1000, 1500, 2500 and 5000, while handling the negations (HN).\n",
    "\n",
    "**NOTE**: The mark negation function takes a about 10 mins to run for each feature size. For 5 feature sizes it takes around 50 mins to get the tfidf vectors with negations marked and another 10 mins to run the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 438.46 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 500, ngram_range=(1, 2), norm='l2')\n",
    "tokens500UB = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 431.92 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1000, ngram_range=(1, 2), norm='l2')\n",
    "tokens1000UB = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 432.46 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 1500, ngram_range=(1, 2), norm='l2')\n",
    "tokens1500UB = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 433.65 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 2500, ngram_range=(1, 2), norm='l2')\n",
    "tokens2500UB = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 429.39 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "tfidf_vect = TfidfVectorizer(analyzer=\"word\", tokenizer=lambda text: mark_negation(word_tokenize(text)), stop_words=\"english\", max_features = 5000, ngram_range=(1, 2), norm='l2')\n",
    "tokens5000UB = tfidf_vect.fit_transform(df_posneg[\"text\"].tolist())\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runClassifiersUBN2C(showRes = False):\n",
    "    \n",
    "    mf_array = [500,1000,1500,2500,5000]\n",
    "    nbdict = {}\n",
    "    svmdict = {}\n",
    "    lrdict = {}\n",
    "    mlpdict = {}\n",
    "    \n",
    "    for MaxFeatures in mf_array:\n",
    "        if MaxFeatures == 500:\n",
    "            tokens = tokens500UB\n",
    "        elif MaxFeatures == 1000:\n",
    "            tokens = tokens1000UB\n",
    "        elif MaxFeatures == 1500:\n",
    "            tokens = tokens1500UB\n",
    "        elif MaxFeatures == 2500:\n",
    "            tokens = tokens2500UB\n",
    "        elif MaxFeatures == 5000:\n",
    "            tokens = tokens5000UB\n",
    "            \n",
    "        print(\"Running classifiers for feature size : {}\".format(MaxFeatures))\n",
    "                \n",
    "        labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "        trainX = vstack([tokens[0:70000], tokens[100000:170000]])\n",
    "        testX = vstack([tokens[70000:100000], tokens[170000:200000]])\n",
    "        trainY = labels[0:70000] + labels[100000:170000] \n",
    "        testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "        # run naive bayes classifier\n",
    "        start_time = timer()\n",
    "        clf = MultinomialNB().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYMNB = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYMNB),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYMNB, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYMNB, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYMNB, average='macro'),3)\n",
    "        nbdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "        \n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYMNB, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)    \n",
    "            print(\"*************Naive Bayes Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run linear SVM classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier().fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYSVC = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "        svmdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Linear SVM Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # run logistic regression classifier\n",
    "        start_time = timer()\n",
    "        clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "        start_time = timer()\n",
    "        predYLR = clf.predict(testX)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "        lrdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predYLR, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            print(\"\\n*************Logitic Regression Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "        # transform the labels into one hot vector form\n",
    "        label_encoder = LabelEncoder()\n",
    "        trainYI = label_encoder.fit_transform(trainY)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "        trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "        testYI = label_encoder.fit_transform(testY)\n",
    "        testYI = testYI.reshape(len(testYI), 1)\n",
    "        testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "        # define the model\n",
    "        x     = Input(shape = (trainX.shape[1], ), dtype = 'float32', sparse = True)     \n",
    "        d1    = Dense(256, activation='relu')(x)\n",
    "        b1    = BatchNormalization()(d1)\n",
    "        d2    = Dropout(0.5)(b1)\n",
    "        d3    = Dense(256, activation='relu')(d2)\n",
    "        b2    = BatchNormalization()(d2)\n",
    "        d4    = Dropout(0.5)(b2)\n",
    "        out   = Dense(2, activation = 'softmax')(d4)\n",
    "        model = Model(x,out)\n",
    "\n",
    "        # train the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        start_time = timer()\n",
    "        model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "        train_time = round(timer()-start_time,2)\n",
    "\n",
    "        # Predict on the test using the trained model\n",
    "        start_time = timer()\n",
    "        pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "        test_time = round(timer()-start_time,2)\n",
    "        # process the predicted probailities to get the final labels encodedLabs\n",
    "        encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "        # flatten the list\n",
    "        predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "        accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "        precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "        recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "        f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "        mlpdict[str(MaxFeatures)] = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "        if showRes:\n",
    "            # Print the performance metrics\n",
    "            print(\"\\n*************MLP Classfier*************\")\n",
    "            print(\"Accuracy: {}\".format(accuracy))\n",
    "            print(\"Macro averaged precision score: {}\".format(precision))\n",
    "            print(\"Macro averaged recall score: {}\".format(recall))\n",
    "            print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "            display(df_confusion)\n",
    "\n",
    "            # print the confusion matrix\n",
    "            test_y = pd.Series(testY, name='Actual')\n",
    "            pred_y = pd.Series(predLabs, name='Predicted')\n",
    "            df_confusion = pd.crosstab(test_y, pred_y)\n",
    "            display(df_confusion)\n",
    "            \n",
    "    return nbdict, svmdict, lrdict, mlpdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifiers for feature size : 500\n",
      "Running classifiers for feature size : 1000\n",
      "Running classifiers for feature size : 1500\n",
      "Running classifiers for feature size : 2500\n",
      "Running classifiers for feature size : 5000\n",
      "Finished in : 422.93 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "nbdict7, svmdict7, lrdict7, mlpdict7 = runClassifiersUBN2C()\n",
    "print(\"Finished in : {} seconds\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL TO LOAD THE SAVED RESULTS\n",
    "### **This assumes the Results folder is in the current working directory\n",
    "\n",
    "# Save the results\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\nbdict.npy\", nbdict7)\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\svmdict.npy\", svmdict7)\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\lrdict.npy\", lrdict7)\n",
    "# np.save(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\mlpdict.npy\", mlpdict7)\n",
    "\n",
    "## to load results from file\n",
    "nbdict7 = np.load(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\nbdict.npy\").item()\n",
    "svmdict7 = np.load(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\svmdict.npy\").item()\n",
    "lrdict7 = np.load(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\lrdict.npy\").item()\n",
    "mlpdict7 = np.load(\"Results\\\\Two_class\\\\UB_SWR_HN\\\\mlpdict.npy\").item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          84.39999999999999,
          86.8,
          87.5,
          88.5,
          89.60000000000001
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.7,
          91.9,
          92.5,
          93.4,
          93.8
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.4,
          91.4,
          91.8,
          92.4,
          92.7
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.7,
          92.7,
          93.10000000000001,
          93.8,
          94.3
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification accuracy for different algorithms (2C_UB_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "Accuracy in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"9e5980b4-d1f1-417e-9933-1183d7532644\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9e5980b4-d1f1-417e-9933-1183d7532644\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [84.39999999999999, 86.8, 87.5, 88.5, 89.60000000000001], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.7, 91.9, 92.5, 93.4, 93.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.4, 91.4, 91.8, 92.4, 92.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.7, 92.7, 93.10000000000001, 93.8, 94.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_UB_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9e5980b4-d1f1-417e-9933-1183d7532644\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9e5980b4-d1f1-417e-9933-1183d7532644\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [84.39999999999999, 86.8, 87.5, 88.5, 89.60000000000001], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.7, 91.9, 92.5, 93.4, 93.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.4, 91.4, 91.8, 92.4, 92.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.7, 92.7, 93.10000000000001, 93.8, 94.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Accuracy in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification accuracy for different algorithms (2C_UB_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict7['500'][0]*100, nbdict7['1000'][0]*100, nbdict7['1500'][0]*100, nbdict7['2500'][0]*100, nbdict7['5000'][0]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict7['500'][0]*100, svmdict7['1000'][0]*100, svmdict7['1500'][0]*100, svmdict7['2500'][0]*100, svmdict7['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict7['500'][0]*100, lrdict7['1000'][0]*100, lrdict7['1500'][0]*100, lrdict7['2500'][0]*100, lrdict7['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict7['500'][0]*100, mlpdict7['1000'][0]*100, mlpdict7['1500'][0]*100, mlpdict7['2500'][0]*100, mlpdict7['5000'][0]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification accuracy for different algorithms (2C_UB_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Accuracy in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "legendgroup": "1",
         "marker": {
          "color": "rgb(0, 93, 171)"
         },
         "mode": "lines+markers",
         "name": "Naive Bayes",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          84.39999999999999,
          86.8,
          87.5,
          88.5,
          89.60000000000001
         ]
        },
        {
         "legendgroup": "2",
         "marker": {
          "color": "rgb(255, 127, 14)"
         },
         "mode": "lines+markers",
         "name": "Linear SVM",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.7,
          91.9,
          92.5,
          93.4,
          93.8
         ]
        },
        {
         "legendgroup": "3",
         "marker": {
          "color": "rgb(74, 174, 74)"
         },
         "mode": "lines+markers",
         "name": "Logistic Regression",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          89.4,
          91.4,
          91.8,
          92.4,
          92.7
         ]
        },
        {
         "legendgroup": "4",
         "marker": {
          "color": "rgb(214, 39, 40)"
         },
         "mode": "lines+markers",
         "name": "Neural net",
         "type": "scatter",
         "x": [
          500,
          1000,
          1500,
          2500,
          5000
         ],
         "y": [
          90.7,
          92.7,
          93.10000000000001,
          93.8,
          94.3
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 700,
        "hovermode": "closest",
        "title": "Comparison of classification F1-score for different algorithms (2C_UB_SWR_HN)",
        "width": 1000,
        "xaxis": {
         "gridwidth": 2,
         "ticklen": 5,
         "title": "Feature size",
         "zeroline": false
        },
        "yaxis": {
         "gridwidth": 2,
         "range": [
          40,
          100
         ],
         "ticklen": 5,
         "title": "F1-score in percentage"
        }
       }
      },
      "text/html": [
       "<div id=\"de1f8761-f5ab-47b5-9677-07212f921f29\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"de1f8761-f5ab-47b5-9677-07212f921f29\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [84.39999999999999, 86.8, 87.5, 88.5, 89.60000000000001], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.7, 91.9, 92.5, 93.4, 93.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.4, 91.4, 91.8, 92.4, 92.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.7, 92.7, 93.10000000000001, 93.8, 94.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_UB_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"de1f8761-f5ab-47b5-9677-07212f921f29\" style=\"height: 700px; width: 1000px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"de1f8761-f5ab-47b5-9677-07212f921f29\", [{\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(0, 93, 171)\"}, \"name\": \"Naive Bayes\", \"x\": [500, 1000, 1500, 2500, 5000], \"mode\": \"lines+markers\", \"y\": [84.39999999999999, 86.8, 87.5, 88.5, 89.60000000000001], \"legendgroup\": \"1\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(255, 127, 14)\"}, \"name\": \"Linear SVM\", \"y\": [89.7, 91.9, 92.5, 93.4, 93.8], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"2\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(74, 174, 74)\"}, \"name\": \"Logistic Regression\", \"y\": [89.4, 91.4, 91.8, 92.4, 92.7], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"3\"}, {\"type\": \"scatter\", \"marker\": {\"color\": \"rgb(214, 39, 40)\"}, \"name\": \"Neural net\", \"y\": [90.7, 92.7, 93.10000000000001, 93.8, 94.3], \"mode\": \"lines+markers\", \"x\": [500, 1000, 1500, 2500, 5000], \"legendgroup\": \"4\"}], {\"width\": 1000, \"yaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"F1-score in percentage\", \"range\": [40, 100]}, \"hovermode\": \"closest\", \"title\": \"Comparison of classification F1-score for different algorithms (2C_UB_SWR_HN)\", \"height\": 700, \"autosize\": false, \"xaxis\": {\"ticklen\": 5, \"gridwidth\": 2, \"title\": \"Feature size\", \"zeroline\": false}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE F1-score\n",
    "# Create traces\n",
    "trace0 = Scatter(\n",
    "    y = [nbdict7['500'][3]*100, nbdict7['1000'][3]*100, nbdict7['1500'][3]*100, nbdict7['2500'][3]*100, nbdict7['5000'][3]*100],\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"1\",\n",
    "    name = 'Naive Bayes',\n",
    "    marker= {'color': 'rgb(0, 93, 171)'}\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [svmdict7['500'][3]*100, svmdict7['1000'][3]*100, svmdict7['1500'][3]*100, svmdict7['2500'][3]*100, svmdict7['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    legendgroup=\"2\",\n",
    "    marker= {'color': 'rgb(255, 127, 14)'},\n",
    "    name = 'Linear SVM'\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [lrdict7['500'][3]*100, lrdict7['1000'][3]*100, lrdict7['1500'][3]*100, lrdict7['2500'][3]*100, lrdict7['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Logistic Regression',\n",
    "    legendgroup=\"3\",\n",
    "    marker= {'color': 'rgb(74, 174, 74)'}\n",
    ")\n",
    "trace3 = Scatter(\n",
    "    x = [500,1000,1500,2500,5000],\n",
    "    y = [mlpdict7['500'][3]*100, mlpdict7['1000'][3]*100, mlpdict7['1500'][3]*100, mlpdict7['2500'][3]*100, mlpdict7['5000'][3]*100],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Neural net',\n",
    "    legendgroup=\"4\",\n",
    "    marker= {'color': 'rgb(214, 39, 40)'}\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    title= 'Comparison of classification F1-score for different algorithms (2C_UB_SWR_HN)',\n",
    "    hovermode= 'closest',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    xaxis= dict(\n",
    "        title= 'Feature size',\n",
    "        ticklen= 5,\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'F1-score in percentage',\n",
    "        ticklen= 5,\n",
    "        range=[40,100],\n",
    "        gridwidth= 2\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "fig = Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run Naïve Bayes, Linear SVM, Logistic Regression and MLP algorithm by using a trained (on the same yelp data) word2vec vector model. The final resulting vector is the frequency weighted average of all word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.916084148148"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the reviews\n",
    "st = timer()\n",
    "tokenized_corpus = [word_tokenize(w) for w in df_posneg[\"text\"]]\n",
    "timer() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.42857916049434"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a word2vec model on the tokenized corpus\n",
    "st = timer()\n",
    "model = gensim.models.Word2Vec(tokenized_corpus, size=100, window=10, min_count=5, workers=12)\n",
    "timer() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in : 113.12 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate word vectors for each review and\n",
    "# get a weighted sum by frequency of words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "start_time = timer()\n",
    "word_vecs = []\n",
    "for i in range(len(df_posneg[\"text\"])):\n",
    "    # Filter out stop words or words not in the trained model\n",
    "    word = [w for w in tokenized_corpus[i] if w not in stop_words and w.isalpha() and w in model.wv.vocab]\n",
    "    C = Counter(word)\n",
    "    if len(word) == 0:\n",
    "        # If none of the words in the review are in the model\n",
    "        # the final vector is assigned as a zero vector\n",
    "        word_vecs.append(np.zeros((100,)))\n",
    "    else:\n",
    "        word_vecs.append(np.average([model.wv.get_vector(w) * C[w] for w in word], axis=0, weights=[C[w] for w in word]))\n",
    "        \n",
    "print(\"Finished in : {} seconds\\n\".format(round(timer()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We skip the naive bayes algorithm as it doesn't work with negative values\n",
    "def runClassifiersW2V2C(showRes = False):\n",
    "    \n",
    "    labels = df_posneg[\"rating\"].tolist()\n",
    "\n",
    "    trainX = np.asarray(word_vecs[0:70000] + word_vecs[100000:170000])\n",
    "    testX = np.asarray(word_vecs[70000:100000] + word_vecs[170000:200000])\n",
    "    trainY = labels[0:70000] + labels[100000:170000] \n",
    "    testY = labels[70000:100000] + labels[170000:200000]\n",
    "\n",
    "    # run linear SVM classifier\n",
    "    start_time = timer()\n",
    "    clf = SGDClassifier().fit(trainX, trainY)\n",
    "    train_time = round(timer()-start_time,2)\n",
    "    start_time = timer()\n",
    "    predYSVC = clf.predict(testX)\n",
    "    test_time = round(timer()-start_time,2)\n",
    "    accuracy = round(sklearn.metrics.accuracy_score(testY, predYSVC),3)\n",
    "    precision = round(sklearn.metrics.precision_score(testY, predYSVC, average='macro'),3)\n",
    "    recall = round(sklearn.metrics.recall_score(testY, predYSVC, average='macro'),3)\n",
    "    f1score = round(sklearn.metrics.f1_score(testY, predYSVC, average='macro'),3)\n",
    "    svmlist = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "    if showRes:\n",
    "        test_y = pd.Series(testY, name='Actual')\n",
    "        pred_y = pd.Series(predYSVC, name='Predicted')\n",
    "        df_confusion = pd.crosstab(test_y, pred_y)\n",
    "        print(\"\\n*************Linear SVM Classfier*************\")\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Macro averaged precision score: {}\".format(precision))\n",
    "        print(\"Macro averaged recall score: {}\".format(recall))\n",
    "        print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "        display(df_confusion)\n",
    "\n",
    "    # run logistic regression classifier\n",
    "    start_time = timer()\n",
    "    clf = SGDClassifier('log').fit(trainX, trainY)\n",
    "    train_time = round(timer()-start_time,2)\n",
    "    start_time = timer()\n",
    "    predYLR = clf.predict(testX)\n",
    "    test_time = round(timer()-start_time,2)\n",
    "    accuracy = round(sklearn.metrics.accuracy_score(testY, predYLR),3)\n",
    "    precision = round(sklearn.metrics.precision_score(testY, predYLR, average='macro'),3)\n",
    "    recall = round(sklearn.metrics.recall_score(testY, predYLR, average='macro'),3)\n",
    "    f1score = round(sklearn.metrics.f1_score(testY, predYLR, average='macro'),3)\n",
    "    lrlist = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "    if showRes:\n",
    "        test_y = pd.Series(testY, name='Actual')\n",
    "        pred_y = pd.Series(predYLR, name='Predicted')\n",
    "        df_confusion = pd.crosstab(test_y, pred_y)\n",
    "        print(\"\\n*************Logistic Regression Classfier*************\")\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Macro averaged precision score: {}\".format(precision))\n",
    "        print(\"Macro averaged recall score: {}\".format(recall))\n",
    "        print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "        display(df_confusion)\n",
    "\n",
    "    # transform the labels into one hot vector form\n",
    "    label_encoder = LabelEncoder()\n",
    "    trainYI = label_encoder.fit_transform(trainY)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    trainYI = trainYI.reshape(len(trainYI), 1)\n",
    "    trainYC = onehot_encoder.fit_transform(trainYI)\n",
    "\n",
    "    testYI = label_encoder.fit_transform(testY)\n",
    "    testYI = testYI.reshape(len(testYI), 1)\n",
    "    testYC = onehot_encoder.fit_transform(testYI)\n",
    "\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=trainX.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, input_dim=50, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    # train the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    start_time = timer()\n",
    "    model.fit(x=trainX, y=trainYC, batch_size=1000, epochs=20, verbose=0)\n",
    "    train_time = round(timer()-start_time,2)\n",
    "\n",
    "    # Predict on the test using the trained model\n",
    "    start_time = timer()\n",
    "    pred_probs = model.predict(x=testX, batch_size=1000, verbose=0)\n",
    "    test_time = round(timer()-start_time,2)\n",
    "    # process the predicted probailities to get the final labels encodedLabs\n",
    "    encodedLabs = (label_encoder.inverse_transform(np.array([[int(p.argmax())] for p in pred_probs], dtype=np.int64))).tolist()\n",
    "    # flatten the list\n",
    "    predLabs = list(itertools.chain.from_iterable(encodedLabs))\n",
    "    accuracy = round(sklearn.metrics.accuracy_score(testY, predLabs),3)\n",
    "    precision = round(sklearn.metrics.precision_score(testY, predLabs, average='macro'),3)\n",
    "    recall = round(sklearn.metrics.recall_score(testY, predLabs, average='macro'),3)\n",
    "    f1score = round(sklearn.metrics.f1_score(testY, predLabs, average='macro'),3)\n",
    "    mlplist = [accuracy, precision, recall, f1score, train_time, test_time]\n",
    "\n",
    "    if showRes:\n",
    "        # Print the performance metrics\n",
    "        print(\"\\n*************MLP Classfier*************\")\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Macro averaged precision score: {}\".format(precision))\n",
    "        print(\"Macro averaged recall score: {}\".format(recall))\n",
    "        print(\"Macro averaged f-1 score: {}\".format(f1score))\n",
    "\n",
    "        # print the confusion matrix\n",
    "        test_y = pd.Series(testY, name='Actual')\n",
    "        pred_y = pd.Series(predLabs, name='Predicted')\n",
    "        df_confusion = pd.crosstab(test_y, pred_y)\n",
    "        display(df_confusion)\n",
    "            \n",
    "    return svmlist, lrlist, mlplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************Linear SVM Classfier*************\n",
      "Accuracy: 0.802\n",
      "Macro averaged precision score: 0.808\n",
      "Macro averaged recall score: 0.802\n",
      "Macro averaged f-1 score: 0.801\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>26139</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8021</td>\n",
       "      <td>21979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  negative  positive\n",
       "Actual                       \n",
       "negative      26139      3861\n",
       "positive       8021     21979"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************Logistic Regression Classfier*************\n",
      "Accuracy: 0.716\n",
      "Macro averaged precision score: 0.771\n",
      "Macro averaged recall score: 0.716\n",
      "Macro averaged f-1 score: 0.701\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>14766</td>\n",
       "      <td>15234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1786</td>\n",
       "      <td>28214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  negative  positive\n",
       "Actual                       \n",
       "negative      14766     15234\n",
       "positive       1786     28214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************MLP Classfier*************\n",
      "Accuracy: 0.843\n",
      "Macro averaged precision score: 0.845\n",
      "Macro averaged recall score: 0.843\n",
      "Macro averaged f-1 score: 0.842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>26586</td>\n",
       "      <td>3414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>6033</td>\n",
       "      <td>23967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  negative  positive\n",
       "Actual                       \n",
       "negative      26586      3414\n",
       "positive       6033     23967"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlist, lrlist, mlplist = runClassifiersW2V2C(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "# np.array(svmlist).dump(open('Results\\\\Two_class\\\\Word2Vec\\\\svmlist.npy', 'wb'))\n",
    "# np.array(lrlist).dump(open('Results\\\\Two_class\\\\Word2Vec\\\\lrlist.npy', 'wb'))\n",
    "# np.array(mlplist).dump(open('Results\\\\Two_class\\\\Word2Vec\\\\mlplist.npy', 'wb'))\n",
    "\n",
    "# Load the results\n",
    "svmlist = np.load(open('Results\\\\Two_class\\\\Word2Vec\\\\svmlist.npy', 'rb'))\n",
    "lrlist = np.load(open('Results\\\\Two_class\\\\Word2Vec\\\\lrlist.npy', 'rb'))\n",
    "mlplist = np.load(open('Results\\\\Two_class\\\\Word2Vec\\\\mlplist.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Linear SVM",
         "opacity": 0.8,
         "text": [
          80.2,
          80.8,
          80.2,
          80.1
         ],
         "textfont": {
          "color": "black",
          "family": "Calibri",
          "size": 14
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score"
         ],
         "y": [
          80.2,
          80.80000000000001,
          80.2,
          80.10000000000001
         ]
        },
        {
         "name": "Logistic Regression",
         "opacity": 0.8,
         "text": [
          71.6,
          77.1,
          71.6,
          70.1
         ],
         "textfont": {
          "color": "black",
          "family": "Calibri",
          "size": 14
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score"
         ],
         "y": [
          71.6,
          77.10000000000001,
          71.6,
          70.1
         ]
        },
        {
         "name": "Neural Net",
         "opacity": 0.8,
         "text": [
          84.3,
          84.5,
          84.3,
          84.2
         ],
         "textfont": {
          "color": "black",
          "family": "Calibri",
          "size": 14
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Accuracy",
          "Precision",
          "Recall",
          "F1-score"
         ],
         "y": [
          84.3,
          84.5,
          84.3,
          84.2
         ]
        }
       ],
       "layout": {
        "title": "Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)",
        "yaxis": {
         "title": "Percent value"
        }
       }
      },
      "text/html": [
       "<div id=\"c431dd19-c07a-4489-b9bb-35ec9335dd3b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c431dd19-c07a-4489-b9bb-35ec9335dd3b\", [{\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Linear SVM\", \"text\": [80.2, 80.8, 80.2, 80.1], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [80.2, 80.80000000000001, 80.2, 80.10000000000001], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Logistic Regression\", \"text\": [71.6, 77.1, 71.6, 70.1], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [71.6, 77.10000000000001, 71.6, 70.1], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Neural Net\", \"text\": [84.3, 84.5, 84.3, 84.2], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [84.3, 84.5, 84.3, 84.2], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}], {\"title\": \"Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)\", \"yaxis\": {\"title\": \"Percent value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c431dd19-c07a-4489-b9bb-35ec9335dd3b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c431dd19-c07a-4489-b9bb-35ec9335dd3b\", [{\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Linear SVM\", \"text\": [80.2, 80.8, 80.2, 80.1], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [80.2, 80.80000000000001, 80.2, 80.10000000000001], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Logistic Regression\", \"text\": [71.6, 77.1, 71.6, 70.1], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [71.6, 77.10000000000001, 71.6, 70.1], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}, {\"type\": \"bar\", \"textposition\": \"auto\", \"name\": \"Neural Net\", \"text\": [84.3, 84.5, 84.3, 84.2], \"x\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"], \"opacity\": 0.8, \"y\": [84.3, 84.5, 84.3, 84.2], \"textfont\": {\"color\": \"black\", \"family\": \"Calibri\", \"size\": 14}}], {\"title\": \"Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)\", \"yaxis\": {\"title\": \"Percent value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## PLOT THE ACCURACY PRECISION RECALL AND F1-SCORE\n",
    "# Create traces\n",
    "trace0 = Bar(\n",
    "    y = [svmlist[0]*100, svmlist[1]*100, svmlist[2]*100, svmlist[3]*100],\n",
    "    x = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "    text = [round(svmlist[0]*100,2), round(svmlist[1]*100,2), round(svmlist[2]*100,2), round(svmlist[3]*100,2)],\n",
    "    textfont=dict(family='Calibri', size=14, color='black'),\n",
    "    textposition = 'auto',\n",
    "    name = 'Linear SVM',\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "trace1 = Bar(\n",
    "    y = [lrlist[0]*100, lrlist[1]*100, lrlist[2]*100, lrlist[3]*100],\n",
    "    x = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "    text = [round(lrlist[0]*100,2), round(lrlist[1]*100,2), round(lrlist[2]*100,2), round(lrlist[3]*100,2)],\n",
    "    textposition = 'auto',\n",
    "    textfont=dict(family='Calibri', size=14, color='black'),\n",
    "    name = 'Logistic Regression',\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "trace2 = Bar(\n",
    "    x = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "    y = [mlplist[0]*100, mlplist[1]*100, mlplist[2]*100, mlplist[3]*100],\n",
    "    text = [round(mlplist[0]*100,2), round(mlplist[1]*100,2), round(mlplist[2]*100,2), round(mlplist[3]*100,2)],\n",
    "    textfont=dict(family='Calibri', size=14, color='black'),\n",
    "    textposition = 'auto',\n",
    "    name = 'Neural Net',\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2]\n",
    "layout = Layout(title=\"Comparing the accuracy, precision, recall and accuracy of different classifiers (W2V_2C)\", \n",
    "               yaxis=dict(title=\"Percent value\"))\n",
    "fig = Figure(data=data,layout=layout)\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
